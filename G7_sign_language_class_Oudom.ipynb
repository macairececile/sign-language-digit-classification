{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sign Language digit classification using RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset: https://www.kaggle.com/ardamavi/sign-language-digits-dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: Recurrent Neural Network for image classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Librairies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import imageio\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Preprocess the data as per the given task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data are images with sign language of numbers from 0 to 9. \n",
    "# Each sign language number is in a specific folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Write custom dataloader and collate function for creating train dataset and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the images\n",
    "path_images = '/Users/prachmeanleakhena/Desktop/IDMC/sign-language-digit-classification/Dataset/'\n",
    "\n",
    "\n",
    "class SignDataset(Dataset):\n",
    "    \"\"\"SignDataset class to read the dataset\n",
    "    Inherit from torch.utils.data.Dataset\n",
    "    functions: init, len, getitem and find_files\"\"\"\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.image_files = self._find_files()\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        elements = list(self.image_files.items())\n",
    "#         print(elements[index][0])\n",
    "        image = io.imread(elements[index][0])\n",
    "        label = torch.tensor(int(elements[index][1]))\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return (image, label)\n",
    "    \n",
    "    \n",
    "    def _find_files(self):\n",
    "        \"\"\"Recursively finds all files matching the pattern.\"\"\"\n",
    "        files = {}\n",
    "        for i in range(0,10):\n",
    "            images = [f for f in listdir(self.root_dir+'/'+str(i)) if isfile(join(self.root_dir+'/'+str(i), f))]\n",
    "            for el in images:\n",
    "                files[self.root_dir+str(i)+'/'+el] = i\n",
    "        return files\n",
    "\n",
    "    \n",
    "class SignCollate(object):\n",
    "    \"\"\"Function object used as a collate function for DataLoader.\"\"\"\n",
    "    def __init__(self, ):\n",
    "        pass\n",
    "        \n",
    "    def _collate_fn(self, batch):\n",
    "        new_batch = []\n",
    "        for idx in range(len(batch)):\n",
    "            sample = batch[idx][0]\n",
    "            label = batch[idx][1]\n",
    "            new_batch.append(sample)\n",
    "\n",
    "        # scalar output\n",
    "        sample_batch = np.array(new_batch)\n",
    "        sample_batch = torch.FloatTensor(sample_batch)\n",
    "       \n",
    "        return sample_batch\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        return self._collate_fn(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2059\n"
     ]
    }
   ],
   "source": [
    "dataset = SignDataset(root_dir=path_images, transform=transforms.ToTensor())  # access to the dataset\n",
    "print(len(dataset))  # 2062 images\n",
    "\n",
    "collate_fn = SignCollate()\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "# Split into TRAIN and TEST\n",
    "train_size = int(0.9 * len(dataset))  # 90% of the data for train set\n",
    "test_size = len(dataset) - train_size  # 10 % of the data for test set\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)\n",
    "\n",
    "# for i, (x,y) in enumerate(train_dataset):\n",
    "#     print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Define the neural network model in pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network model -> Recurrent Neural Network (RNN)\n",
    "class RNN_LSTM_classifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(RNN_LSTM_classifier, self).__init__()\n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.out = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        r_out, (_, _) = self.rnn(x, None)\n",
    "        out = self.out(r_out[:, -1, :])\n",
    "        return out\n",
    "    \n",
    "#         super(RNN_LSTM_classifier, self).__init__()\n",
    "#         self.hidden_size = hidden_size\n",
    "#         self.num_layers = num_layers\n",
    "#         self.num_direction = num_direction\n",
    "        \n",
    "#         if self.num_direction == 1:\n",
    "#             self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=False)\n",
    "#         else:\n",
    "#             self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "        \n",
    "#         self.fc = nn.Linear(hidden_size*self.num_direction, num_classes)\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         # batch_size x seq_len x input_size\n",
    "#         # Forward propagate LSTM\n",
    "#         out, _ = self.lstm(x)  # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
    "        \n",
    "#         # Decode the hidden state of the last time step\n",
    "#         out = self.fc(out[:, -1, :])\n",
    "#         return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4. Define hyperparameters to create instance of neural network model as well as parameters required to train neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "num_layers = 2\n",
    "sequence_length = 32\n",
    "input_size = 100\n",
    "hidden_size = 64\n",
    "num_classes = 10   # 10 classes (digits from 0 to 9)\n",
    "num_epochs = 500   # for the training\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5. Write training loop for training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN_LSTM_classifier(input_size, hidden_size,num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/500], Step [10/58], Loss: 2.3417\n",
      "Epoch [1/500], Step [20/58], Loss: 2.2931\n",
      "Epoch [1/500], Step [30/58], Loss: 2.2850\n",
      "Epoch [1/500], Step [40/58], Loss: 2.1175\n",
      "Epoch [1/500], Step [50/58], Loss: 2.2855\n",
      "Epoch [2/500], Step [10/58], Loss: 2.0901\n",
      "Epoch [2/500], Step [20/58], Loss: 2.2693\n",
      "Epoch [2/500], Step [30/58], Loss: 2.3215\n",
      "Epoch [2/500], Step [40/58], Loss: 2.2540\n",
      "Epoch [2/500], Step [50/58], Loss: 2.2539\n",
      "Epoch [3/500], Step [10/58], Loss: 2.0263\n",
      "Epoch [3/500], Step [20/58], Loss: 2.2329\n",
      "Epoch [3/500], Step [30/58], Loss: 2.1853\n",
      "Epoch [3/500], Step [40/58], Loss: 2.1102\n",
      "Epoch [3/500], Step [50/58], Loss: 2.1149\n",
      "Epoch [4/500], Step [10/58], Loss: 2.4935\n",
      "Epoch [4/500], Step [20/58], Loss: 2.2666\n",
      "Epoch [4/500], Step [30/58], Loss: 2.1955\n",
      "Epoch [4/500], Step [40/58], Loss: 2.2020\n",
      "Epoch [4/500], Step [50/58], Loss: 2.2186\n",
      "Epoch [5/500], Step [10/58], Loss: 2.1736\n",
      "Epoch [5/500], Step [20/58], Loss: 2.1380\n",
      "Epoch [5/500], Step [30/58], Loss: 2.1636\n",
      "Epoch [5/500], Step [40/58], Loss: 2.0610\n",
      "Epoch [5/500], Step [50/58], Loss: 2.0760\n",
      "Epoch [6/500], Step [10/58], Loss: 2.2592\n",
      "Epoch [6/500], Step [20/58], Loss: 2.0088\n",
      "Epoch [6/500], Step [30/58], Loss: 2.1623\n",
      "Epoch [6/500], Step [40/58], Loss: 2.1277\n",
      "Epoch [6/500], Step [50/58], Loss: 1.8991\n",
      "Epoch [7/500], Step [10/58], Loss: 2.0997\n",
      "Epoch [7/500], Step [20/58], Loss: 2.1906\n",
      "Epoch [7/500], Step [30/58], Loss: 2.2697\n",
      "Epoch [7/500], Step [40/58], Loss: 2.1869\n",
      "Epoch [7/500], Step [50/58], Loss: 2.1836\n",
      "Epoch [8/500], Step [10/58], Loss: 2.0182\n",
      "Epoch [8/500], Step [20/58], Loss: 2.0994\n",
      "Epoch [8/500], Step [30/58], Loss: 2.4323\n",
      "Epoch [8/500], Step [40/58], Loss: 2.1253\n",
      "Epoch [8/500], Step [50/58], Loss: 2.0574\n",
      "Epoch [9/500], Step [10/58], Loss: 2.2283\n",
      "Epoch [9/500], Step [20/58], Loss: 1.9982\n",
      "Epoch [9/500], Step [30/58], Loss: 2.1943\n",
      "Epoch [9/500], Step [40/58], Loss: 2.1991\n",
      "Epoch [9/500], Step [50/58], Loss: 1.8550\n",
      "Epoch [10/500], Step [10/58], Loss: 2.1265\n",
      "Epoch [10/500], Step [20/58], Loss: 2.1702\n",
      "Epoch [10/500], Step [30/58], Loss: 1.9190\n",
      "Epoch [10/500], Step [40/58], Loss: 2.2173\n",
      "Epoch [10/500], Step [50/58], Loss: 2.3173\n",
      "Epoch [11/500], Step [10/58], Loss: 2.2567\n",
      "Epoch [11/500], Step [20/58], Loss: 2.1699\n",
      "Epoch [11/500], Step [30/58], Loss: 2.0442\n",
      "Epoch [11/500], Step [40/58], Loss: 1.9591\n",
      "Epoch [11/500], Step [50/58], Loss: 2.1306\n",
      "Epoch [12/500], Step [10/58], Loss: 2.0021\n",
      "Epoch [12/500], Step [20/58], Loss: 2.1463\n",
      "Epoch [12/500], Step [30/58], Loss: 2.0297\n",
      "Epoch [12/500], Step [40/58], Loss: 2.1079\n",
      "Epoch [12/500], Step [50/58], Loss: 2.3127\n",
      "Epoch [13/500], Step [10/58], Loss: 2.2822\n",
      "Epoch [13/500], Step [20/58], Loss: 2.1518\n",
      "Epoch [13/500], Step [30/58], Loss: 2.2619\n",
      "Epoch [13/500], Step [40/58], Loss: 1.8577\n",
      "Epoch [13/500], Step [50/58], Loss: 2.2047\n",
      "Epoch [14/500], Step [10/58], Loss: 2.0709\n",
      "Epoch [14/500], Step [20/58], Loss: 1.8564\n",
      "Epoch [14/500], Step [30/58], Loss: 1.7798\n",
      "Epoch [14/500], Step [40/58], Loss: 1.9489\n",
      "Epoch [14/500], Step [50/58], Loss: 2.1710\n",
      "Epoch [15/500], Step [10/58], Loss: 1.9509\n",
      "Epoch [15/500], Step [20/58], Loss: 2.0346\n",
      "Epoch [15/500], Step [30/58], Loss: 2.2315\n",
      "Epoch [15/500], Step [40/58], Loss: 2.0932\n",
      "Epoch [15/500], Step [50/58], Loss: 1.9034\n",
      "Epoch [16/500], Step [10/58], Loss: 2.1401\n",
      "Epoch [16/500], Step [20/58], Loss: 2.2985\n",
      "Epoch [16/500], Step [30/58], Loss: 2.0336\n",
      "Epoch [16/500], Step [40/58], Loss: 1.9691\n",
      "Epoch [16/500], Step [50/58], Loss: 1.9328\n",
      "Epoch [17/500], Step [10/58], Loss: 1.8089\n",
      "Epoch [17/500], Step [20/58], Loss: 2.0644\n",
      "Epoch [17/500], Step [30/58], Loss: 1.9370\n",
      "Epoch [17/500], Step [40/58], Loss: 1.9682\n",
      "Epoch [17/500], Step [50/58], Loss: 1.8587\n",
      "Epoch [18/500], Step [10/58], Loss: 1.9255\n",
      "Epoch [18/500], Step [20/58], Loss: 2.0164\n",
      "Epoch [18/500], Step [30/58], Loss: 2.0561\n",
      "Epoch [18/500], Step [40/58], Loss: 2.0119\n",
      "Epoch [18/500], Step [50/58], Loss: 1.9790\n",
      "Epoch [19/500], Step [10/58], Loss: 2.1091\n",
      "Epoch [19/500], Step [20/58], Loss: 1.9476\n",
      "Epoch [19/500], Step [30/58], Loss: 1.9158\n",
      "Epoch [19/500], Step [40/58], Loss: 1.9824\n",
      "Epoch [19/500], Step [50/58], Loss: 2.0619\n",
      "Epoch [20/500], Step [10/58], Loss: 1.7517\n",
      "Epoch [20/500], Step [20/58], Loss: 1.7419\n",
      "Epoch [20/500], Step [30/58], Loss: 1.8128\n",
      "Epoch [20/500], Step [40/58], Loss: 2.0919\n",
      "Epoch [20/500], Step [50/58], Loss: 2.0280\n",
      "Epoch [21/500], Step [10/58], Loss: 1.9418\n",
      "Epoch [21/500], Step [20/58], Loss: 1.8638\n",
      "Epoch [21/500], Step [30/58], Loss: 1.9092\n",
      "Epoch [21/500], Step [40/58], Loss: 1.9922\n",
      "Epoch [21/500], Step [50/58], Loss: 2.0107\n",
      "Epoch [22/500], Step [10/58], Loss: 1.9681\n",
      "Epoch [22/500], Step [20/58], Loss: 1.9333\n",
      "Epoch [22/500], Step [30/58], Loss: 1.9545\n",
      "Epoch [22/500], Step [40/58], Loss: 2.1148\n",
      "Epoch [22/500], Step [50/58], Loss: 1.6921\n",
      "Epoch [23/500], Step [10/58], Loss: 1.9241\n",
      "Epoch [23/500], Step [20/58], Loss: 1.8860\n",
      "Epoch [23/500], Step [30/58], Loss: 1.9904\n",
      "Epoch [23/500], Step [40/58], Loss: 2.0158\n",
      "Epoch [23/500], Step [50/58], Loss: 1.6636\n",
      "Epoch [24/500], Step [10/58], Loss: 1.7325\n",
      "Epoch [24/500], Step [20/58], Loss: 1.9469\n",
      "Epoch [24/500], Step [30/58], Loss: 1.9248\n",
      "Epoch [24/500], Step [40/58], Loss: 2.0377\n",
      "Epoch [24/500], Step [50/58], Loss: 1.8293\n",
      "Epoch [25/500], Step [10/58], Loss: 1.8724\n",
      "Epoch [25/500], Step [20/58], Loss: 2.0359\n",
      "Epoch [25/500], Step [30/58], Loss: 1.9684\n",
      "Epoch [25/500], Step [40/58], Loss: 1.8384\n",
      "Epoch [25/500], Step [50/58], Loss: 2.3294\n",
      "Epoch [26/500], Step [10/58], Loss: 1.8845\n",
      "Epoch [26/500], Step [20/58], Loss: 1.8969\n",
      "Epoch [26/500], Step [30/58], Loss: 1.7541\n",
      "Epoch [26/500], Step [40/58], Loss: 1.9629\n",
      "Epoch [26/500], Step [50/58], Loss: 1.8695\n",
      "Epoch [27/500], Step [10/58], Loss: 1.9575\n",
      "Epoch [27/500], Step [20/58], Loss: 1.7727\n",
      "Epoch [27/500], Step [30/58], Loss: 1.8660\n",
      "Epoch [27/500], Step [40/58], Loss: 2.2807\n",
      "Epoch [27/500], Step [50/58], Loss: 1.9522\n",
      "Epoch [28/500], Step [10/58], Loss: 1.7937\n",
      "Epoch [28/500], Step [20/58], Loss: 1.6935\n",
      "Epoch [28/500], Step [30/58], Loss: 1.7513\n",
      "Epoch [28/500], Step [40/58], Loss: 1.8047\n",
      "Epoch [28/500], Step [50/58], Loss: 2.1028\n",
      "Epoch [29/500], Step [10/58], Loss: 1.6811\n",
      "Epoch [29/500], Step [20/58], Loss: 1.9318\n",
      "Epoch [29/500], Step [30/58], Loss: 1.7638\n",
      "Epoch [29/500], Step [40/58], Loss: 1.6563\n",
      "Epoch [29/500], Step [50/58], Loss: 2.0447\n",
      "Epoch [30/500], Step [10/58], Loss: 1.7339\n",
      "Epoch [30/500], Step [20/58], Loss: 1.7659\n",
      "Epoch [30/500], Step [30/58], Loss: 1.6354\n",
      "Epoch [30/500], Step [40/58], Loss: 1.8161\n",
      "Epoch [30/500], Step [50/58], Loss: 1.5243\n",
      "Epoch [31/500], Step [10/58], Loss: 1.6509\n",
      "Epoch [31/500], Step [20/58], Loss: 1.4188\n",
      "Epoch [31/500], Step [30/58], Loss: 1.6943\n",
      "Epoch [31/500], Step [40/58], Loss: 1.7040\n",
      "Epoch [31/500], Step [50/58], Loss: 1.7478\n",
      "Epoch [32/500], Step [10/58], Loss: 1.5191\n",
      "Epoch [32/500], Step [20/58], Loss: 1.8837\n",
      "Epoch [32/500], Step [30/58], Loss: 1.7554\n",
      "Epoch [32/500], Step [40/58], Loss: 1.4921\n",
      "Epoch [32/500], Step [50/58], Loss: 2.2043\n",
      "Epoch [33/500], Step [10/58], Loss: 1.5463\n",
      "Epoch [33/500], Step [20/58], Loss: 1.6669\n",
      "Epoch [33/500], Step [30/58], Loss: 2.1443\n",
      "Epoch [33/500], Step [40/58], Loss: 2.0602\n",
      "Epoch [33/500], Step [50/58], Loss: 1.8576\n",
      "Epoch [34/500], Step [10/58], Loss: 1.7162\n",
      "Epoch [34/500], Step [20/58], Loss: 1.7987\n",
      "Epoch [34/500], Step [30/58], Loss: 1.9384\n",
      "Epoch [34/500], Step [40/58], Loss: 1.8611\n",
      "Epoch [34/500], Step [50/58], Loss: 1.7634\n",
      "Epoch [35/500], Step [10/58], Loss: 1.6069\n",
      "Epoch [35/500], Step [20/58], Loss: 1.5617\n",
      "Epoch [35/500], Step [30/58], Loss: 1.9601\n",
      "Epoch [35/500], Step [40/58], Loss: 1.7155\n",
      "Epoch [35/500], Step [50/58], Loss: 1.7228\n",
      "Epoch [36/500], Step [10/58], Loss: 1.7335\n",
      "Epoch [36/500], Step [20/58], Loss: 1.5848\n",
      "Epoch [36/500], Step [30/58], Loss: 1.8159\n",
      "Epoch [36/500], Step [40/58], Loss: 1.6737\n",
      "Epoch [36/500], Step [50/58], Loss: 1.6101\n",
      "Epoch [37/500], Step [10/58], Loss: 1.7157\n",
      "Epoch [37/500], Step [20/58], Loss: 1.8454\n",
      "Epoch [37/500], Step [30/58], Loss: 2.0009\n",
      "Epoch [37/500], Step [40/58], Loss: 1.9165\n",
      "Epoch [37/500], Step [50/58], Loss: 1.5675\n",
      "Epoch [38/500], Step [10/58], Loss: 1.6749\n",
      "Epoch [38/500], Step [20/58], Loss: 1.2977\n",
      "Epoch [38/500], Step [30/58], Loss: 1.5423\n",
      "Epoch [38/500], Step [40/58], Loss: 1.6036\n",
      "Epoch [38/500], Step [50/58], Loss: 1.6006\n",
      "Epoch [39/500], Step [10/58], Loss: 1.5009\n",
      "Epoch [39/500], Step [20/58], Loss: 1.6796\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [39/500], Step [30/58], Loss: 1.8218\n",
      "Epoch [39/500], Step [40/58], Loss: 1.1726\n",
      "Epoch [39/500], Step [50/58], Loss: 1.8377\n",
      "Epoch [40/500], Step [10/58], Loss: 1.7018\n",
      "Epoch [40/500], Step [20/58], Loss: 1.6134\n",
      "Epoch [40/500], Step [30/58], Loss: 1.6641\n",
      "Epoch [40/500], Step [40/58], Loss: 1.7044\n",
      "Epoch [40/500], Step [50/58], Loss: 1.6927\n",
      "Epoch [41/500], Step [10/58], Loss: 1.4861\n",
      "Epoch [41/500], Step [20/58], Loss: 1.7303\n",
      "Epoch [41/500], Step [30/58], Loss: 1.4278\n",
      "Epoch [41/500], Step [40/58], Loss: 1.9049\n",
      "Epoch [41/500], Step [50/58], Loss: 1.7321\n",
      "Epoch [42/500], Step [10/58], Loss: 1.5094\n",
      "Epoch [42/500], Step [20/58], Loss: 1.3145\n",
      "Epoch [42/500], Step [30/58], Loss: 2.0412\n",
      "Epoch [42/500], Step [40/58], Loss: 1.3287\n",
      "Epoch [42/500], Step [50/58], Loss: 1.2962\n",
      "Epoch [43/500], Step [10/58], Loss: 1.4249\n",
      "Epoch [43/500], Step [20/58], Loss: 1.4261\n",
      "Epoch [43/500], Step [30/58], Loss: 1.4001\n",
      "Epoch [43/500], Step [40/58], Loss: 1.4734\n",
      "Epoch [43/500], Step [50/58], Loss: 1.4091\n",
      "Epoch [44/500], Step [10/58], Loss: 1.2536\n",
      "Epoch [44/500], Step [20/58], Loss: 1.5647\n",
      "Epoch [44/500], Step [30/58], Loss: 1.6030\n",
      "Epoch [44/500], Step [40/58], Loss: 1.4966\n",
      "Epoch [44/500], Step [50/58], Loss: 1.4431\n",
      "Epoch [45/500], Step [10/58], Loss: 1.2199\n",
      "Epoch [45/500], Step [20/58], Loss: 1.2493\n",
      "Epoch [45/500], Step [30/58], Loss: 1.3445\n",
      "Epoch [45/500], Step [40/58], Loss: 1.3692\n",
      "Epoch [45/500], Step [50/58], Loss: 1.4128\n",
      "Epoch [46/500], Step [10/58], Loss: 1.4323\n",
      "Epoch [46/500], Step [20/58], Loss: 1.3381\n",
      "Epoch [46/500], Step [30/58], Loss: 1.6062\n",
      "Epoch [46/500], Step [40/58], Loss: 1.3570\n",
      "Epoch [46/500], Step [50/58], Loss: 1.3389\n",
      "Epoch [47/500], Step [10/58], Loss: 1.3898\n",
      "Epoch [47/500], Step [20/58], Loss: 1.5378\n",
      "Epoch [47/500], Step [30/58], Loss: 1.4843\n",
      "Epoch [47/500], Step [40/58], Loss: 1.4393\n",
      "Epoch [47/500], Step [50/58], Loss: 1.2515\n",
      "Epoch [48/500], Step [10/58], Loss: 1.2733\n",
      "Epoch [48/500], Step [20/58], Loss: 1.3393\n",
      "Epoch [48/500], Step [30/58], Loss: 1.6346\n",
      "Epoch [48/500], Step [40/58], Loss: 1.2777\n",
      "Epoch [48/500], Step [50/58], Loss: 1.3829\n",
      "Epoch [49/500], Step [10/58], Loss: 1.3058\n",
      "Epoch [49/500], Step [20/58], Loss: 1.5150\n",
      "Epoch [49/500], Step [30/58], Loss: 1.1756\n",
      "Epoch [49/500], Step [40/58], Loss: 1.3320\n",
      "Epoch [49/500], Step [50/58], Loss: 1.6558\n",
      "Epoch [50/500], Step [10/58], Loss: 1.5123\n",
      "Epoch [50/500], Step [20/58], Loss: 1.3824\n",
      "Epoch [50/500], Step [30/58], Loss: 1.3375\n",
      "Epoch [50/500], Step [40/58], Loss: 1.6112\n",
      "Epoch [50/500], Step [50/58], Loss: 1.3941\n",
      "Epoch [51/500], Step [10/58], Loss: 1.6240\n",
      "Epoch [51/500], Step [20/58], Loss: 1.3208\n",
      "Epoch [51/500], Step [30/58], Loss: 1.3600\n",
      "Epoch [51/500], Step [40/58], Loss: 1.1414\n",
      "Epoch [51/500], Step [50/58], Loss: 1.3819\n",
      "Epoch [52/500], Step [10/58], Loss: 1.3634\n",
      "Epoch [52/500], Step [20/58], Loss: 1.1590\n",
      "Epoch [52/500], Step [30/58], Loss: 1.2744\n",
      "Epoch [52/500], Step [40/58], Loss: 1.1079\n",
      "Epoch [52/500], Step [50/58], Loss: 1.3862\n",
      "Epoch [53/500], Step [10/58], Loss: 1.0122\n",
      "Epoch [53/500], Step [20/58], Loss: 1.3771\n",
      "Epoch [53/500], Step [30/58], Loss: 1.4489\n",
      "Epoch [53/500], Step [40/58], Loss: 1.3252\n",
      "Epoch [53/500], Step [50/58], Loss: 1.0688\n",
      "Epoch [54/500], Step [10/58], Loss: 1.2339\n",
      "Epoch [54/500], Step [20/58], Loss: 1.1068\n",
      "Epoch [54/500], Step [30/58], Loss: 1.3804\n",
      "Epoch [54/500], Step [40/58], Loss: 1.4873\n",
      "Epoch [54/500], Step [50/58], Loss: 1.2765\n",
      "Epoch [55/500], Step [10/58], Loss: 1.1869\n",
      "Epoch [55/500], Step [20/58], Loss: 0.9939\n",
      "Epoch [55/500], Step [30/58], Loss: 1.1477\n",
      "Epoch [55/500], Step [40/58], Loss: 1.4118\n",
      "Epoch [55/500], Step [50/58], Loss: 1.0949\n",
      "Epoch [56/500], Step [10/58], Loss: 1.2665\n",
      "Epoch [56/500], Step [20/58], Loss: 1.0738\n",
      "Epoch [56/500], Step [30/58], Loss: 1.4427\n",
      "Epoch [56/500], Step [40/58], Loss: 1.5049\n",
      "Epoch [56/500], Step [50/58], Loss: 1.0160\n",
      "Epoch [57/500], Step [10/58], Loss: 1.0783\n",
      "Epoch [57/500], Step [20/58], Loss: 1.2011\n",
      "Epoch [57/500], Step [30/58], Loss: 0.7460\n",
      "Epoch [57/500], Step [40/58], Loss: 1.0044\n",
      "Epoch [57/500], Step [50/58], Loss: 1.2818\n",
      "Epoch [58/500], Step [10/58], Loss: 1.0754\n",
      "Epoch [58/500], Step [20/58], Loss: 1.0146\n",
      "Epoch [58/500], Step [30/58], Loss: 0.8720\n",
      "Epoch [58/500], Step [40/58], Loss: 1.1889\n",
      "Epoch [58/500], Step [50/58], Loss: 1.0728\n",
      "Epoch [59/500], Step [10/58], Loss: 1.1314\n",
      "Epoch [59/500], Step [20/58], Loss: 1.1561\n",
      "Epoch [59/500], Step [30/58], Loss: 1.1946\n",
      "Epoch [59/500], Step [40/58], Loss: 1.0715\n",
      "Epoch [59/500], Step [50/58], Loss: 1.5522\n",
      "Epoch [60/500], Step [10/58], Loss: 1.0489\n",
      "Epoch [60/500], Step [20/58], Loss: 1.2325\n",
      "Epoch [60/500], Step [30/58], Loss: 1.1403\n",
      "Epoch [60/500], Step [40/58], Loss: 1.2329\n",
      "Epoch [60/500], Step [50/58], Loss: 1.0031\n",
      "Epoch [61/500], Step [10/58], Loss: 1.7062\n",
      "Epoch [61/500], Step [20/58], Loss: 1.4559\n",
      "Epoch [61/500], Step [30/58], Loss: 1.5122\n",
      "Epoch [61/500], Step [40/58], Loss: 1.8139\n",
      "Epoch [61/500], Step [50/58], Loss: 1.5521\n",
      "Epoch [62/500], Step [10/58], Loss: 1.1209\n",
      "Epoch [62/500], Step [20/58], Loss: 1.3472\n",
      "Epoch [62/500], Step [30/58], Loss: 1.5492\n",
      "Epoch [62/500], Step [40/58], Loss: 1.0425\n",
      "Epoch [62/500], Step [50/58], Loss: 1.3105\n",
      "Epoch [63/500], Step [10/58], Loss: 1.0140\n",
      "Epoch [63/500], Step [20/58], Loss: 1.1709\n",
      "Epoch [63/500], Step [30/58], Loss: 1.2809\n",
      "Epoch [63/500], Step [40/58], Loss: 0.9865\n",
      "Epoch [63/500], Step [50/58], Loss: 1.3179\n",
      "Epoch [64/500], Step [10/58], Loss: 1.4020\n",
      "Epoch [64/500], Step [20/58], Loss: 1.1453\n",
      "Epoch [64/500], Step [30/58], Loss: 1.1251\n",
      "Epoch [64/500], Step [40/58], Loss: 1.1326\n",
      "Epoch [64/500], Step [50/58], Loss: 0.8829\n",
      "Epoch [65/500], Step [10/58], Loss: 1.3246\n",
      "Epoch [65/500], Step [20/58], Loss: 1.1915\n",
      "Epoch [65/500], Step [30/58], Loss: 1.0989\n",
      "Epoch [65/500], Step [40/58], Loss: 1.3568\n",
      "Epoch [65/500], Step [50/58], Loss: 1.3436\n",
      "Epoch [66/500], Step [10/58], Loss: 1.5500\n",
      "Epoch [66/500], Step [20/58], Loss: 1.2592\n",
      "Epoch [66/500], Step [30/58], Loss: 1.0266\n",
      "Epoch [66/500], Step [40/58], Loss: 0.9062\n",
      "Epoch [66/500], Step [50/58], Loss: 0.7415\n",
      "Epoch [67/500], Step [10/58], Loss: 1.1047\n",
      "Epoch [67/500], Step [20/58], Loss: 1.1645\n",
      "Epoch [67/500], Step [30/58], Loss: 0.9173\n",
      "Epoch [67/500], Step [40/58], Loss: 1.1435\n",
      "Epoch [67/500], Step [50/58], Loss: 0.8796\n",
      "Epoch [68/500], Step [10/58], Loss: 0.9544\n",
      "Epoch [68/500], Step [20/58], Loss: 0.9692\n",
      "Epoch [68/500], Step [30/58], Loss: 0.7319\n",
      "Epoch [68/500], Step [40/58], Loss: 1.2228\n",
      "Epoch [68/500], Step [50/58], Loss: 1.1142\n",
      "Epoch [69/500], Step [10/58], Loss: 0.9514\n",
      "Epoch [69/500], Step [20/58], Loss: 0.7686\n",
      "Epoch [69/500], Step [30/58], Loss: 1.1653\n",
      "Epoch [69/500], Step [40/58], Loss: 1.3697\n",
      "Epoch [69/500], Step [50/58], Loss: 0.8346\n",
      "Epoch [70/500], Step [10/58], Loss: 0.9142\n",
      "Epoch [70/500], Step [20/58], Loss: 0.9954\n",
      "Epoch [70/500], Step [30/58], Loss: 1.2818\n",
      "Epoch [70/500], Step [40/58], Loss: 1.0394\n",
      "Epoch [70/500], Step [50/58], Loss: 0.9501\n",
      "Epoch [71/500], Step [10/58], Loss: 0.7632\n",
      "Epoch [71/500], Step [20/58], Loss: 1.1431\n",
      "Epoch [71/500], Step [30/58], Loss: 1.1826\n",
      "Epoch [71/500], Step [40/58], Loss: 0.9517\n",
      "Epoch [71/500], Step [50/58], Loss: 1.0401\n",
      "Epoch [72/500], Step [10/58], Loss: 0.8607\n",
      "Epoch [72/500], Step [20/58], Loss: 0.6328\n",
      "Epoch [72/500], Step [30/58], Loss: 0.7087\n",
      "Epoch [72/500], Step [40/58], Loss: 0.9124\n",
      "Epoch [72/500], Step [50/58], Loss: 1.0300\n",
      "Epoch [73/500], Step [10/58], Loss: 0.9278\n",
      "Epoch [73/500], Step [20/58], Loss: 0.7633\n",
      "Epoch [73/500], Step [30/58], Loss: 0.6067\n",
      "Epoch [73/500], Step [40/58], Loss: 0.8344\n",
      "Epoch [73/500], Step [50/58], Loss: 0.8214\n",
      "Epoch [74/500], Step [10/58], Loss: 0.8532\n",
      "Epoch [74/500], Step [20/58], Loss: 0.6523\n",
      "Epoch [74/500], Step [30/58], Loss: 0.9188\n",
      "Epoch [74/500], Step [40/58], Loss: 1.1279\n",
      "Epoch [74/500], Step [50/58], Loss: 0.6371\n",
      "Epoch [75/500], Step [10/58], Loss: 0.6394\n",
      "Epoch [75/500], Step [20/58], Loss: 0.7727\n",
      "Epoch [75/500], Step [30/58], Loss: 0.6743\n",
      "Epoch [75/500], Step [40/58], Loss: 0.8420\n",
      "Epoch [75/500], Step [50/58], Loss: 0.7462\n",
      "Epoch [76/500], Step [10/58], Loss: 0.7635\n",
      "Epoch [76/500], Step [20/58], Loss: 0.8291\n",
      "Epoch [76/500], Step [30/58], Loss: 0.7301\n",
      "Epoch [76/500], Step [40/58], Loss: 0.6559\n",
      "Epoch [76/500], Step [50/58], Loss: 0.5962\n",
      "Epoch [77/500], Step [10/58], Loss: 0.6149\n",
      "Epoch [77/500], Step [20/58], Loss: 0.6315\n",
      "Epoch [77/500], Step [30/58], Loss: 0.8386\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [77/500], Step [40/58], Loss: 0.7507\n",
      "Epoch [77/500], Step [50/58], Loss: 0.6523\n",
      "Epoch [78/500], Step [10/58], Loss: 0.7447\n",
      "Epoch [78/500], Step [20/58], Loss: 0.8212\n",
      "Epoch [78/500], Step [30/58], Loss: 0.7903\n",
      "Epoch [78/500], Step [40/58], Loss: 0.9064\n",
      "Epoch [78/500], Step [50/58], Loss: 0.4717\n",
      "Epoch [79/500], Step [10/58], Loss: 0.5339\n",
      "Epoch [79/500], Step [20/58], Loss: 0.7841\n",
      "Epoch [79/500], Step [30/58], Loss: 0.4738\n",
      "Epoch [79/500], Step [40/58], Loss: 0.9392\n",
      "Epoch [79/500], Step [50/58], Loss: 1.0740\n",
      "Epoch [80/500], Step [10/58], Loss: 0.8955\n",
      "Epoch [80/500], Step [20/58], Loss: 0.5894\n",
      "Epoch [80/500], Step [30/58], Loss: 0.6245\n",
      "Epoch [80/500], Step [40/58], Loss: 0.6677\n",
      "Epoch [80/500], Step [50/58], Loss: 0.4985\n",
      "Epoch [81/500], Step [10/58], Loss: 0.8052\n",
      "Epoch [81/500], Step [20/58], Loss: 0.6193\n",
      "Epoch [81/500], Step [30/58], Loss: 0.5815\n",
      "Epoch [81/500], Step [40/58], Loss: 0.4774\n",
      "Epoch [81/500], Step [50/58], Loss: 0.7790\n",
      "Epoch [82/500], Step [10/58], Loss: 0.5353\n",
      "Epoch [82/500], Step [20/58], Loss: 0.5990\n",
      "Epoch [82/500], Step [30/58], Loss: 0.4982\n",
      "Epoch [82/500], Step [40/58], Loss: 0.7050\n",
      "Epoch [82/500], Step [50/58], Loss: 0.3756\n",
      "Epoch [83/500], Step [10/58], Loss: 0.4678\n",
      "Epoch [83/500], Step [20/58], Loss: 0.6470\n",
      "Epoch [83/500], Step [30/58], Loss: 0.4334\n",
      "Epoch [83/500], Step [40/58], Loss: 0.6333\n",
      "Epoch [83/500], Step [50/58], Loss: 0.5221\n",
      "Epoch [84/500], Step [10/58], Loss: 0.3848\n",
      "Epoch [84/500], Step [20/58], Loss: 0.7476\n",
      "Epoch [84/500], Step [30/58], Loss: 0.4480\n",
      "Epoch [84/500], Step [40/58], Loss: 0.5201\n",
      "Epoch [84/500], Step [50/58], Loss: 0.4964\n",
      "Epoch [85/500], Step [10/58], Loss: 0.6020\n",
      "Epoch [85/500], Step [20/58], Loss: 0.4573\n",
      "Epoch [85/500], Step [30/58], Loss: 0.4048\n",
      "Epoch [85/500], Step [40/58], Loss: 0.4345\n",
      "Epoch [85/500], Step [50/58], Loss: 0.4687\n",
      "Epoch [86/500], Step [10/58], Loss: 0.2903\n",
      "Epoch [86/500], Step [20/58], Loss: 0.7779\n",
      "Epoch [86/500], Step [30/58], Loss: 0.7711\n",
      "Epoch [86/500], Step [40/58], Loss: 0.5401\n",
      "Epoch [86/500], Step [50/58], Loss: 0.5922\n",
      "Epoch [87/500], Step [10/58], Loss: 0.4639\n",
      "Epoch [87/500], Step [20/58], Loss: 0.7746\n",
      "Epoch [87/500], Step [30/58], Loss: 0.4958\n",
      "Epoch [87/500], Step [40/58], Loss: 0.4378\n",
      "Epoch [87/500], Step [50/58], Loss: 0.3499\n",
      "Epoch [88/500], Step [10/58], Loss: 0.4801\n",
      "Epoch [88/500], Step [20/58], Loss: 0.4285\n",
      "Epoch [88/500], Step [30/58], Loss: 0.4454\n",
      "Epoch [88/500], Step [40/58], Loss: 0.4259\n",
      "Epoch [88/500], Step [50/58], Loss: 0.4735\n",
      "Epoch [89/500], Step [10/58], Loss: 0.3226\n",
      "Epoch [89/500], Step [20/58], Loss: 0.3288\n",
      "Epoch [89/500], Step [30/58], Loss: 0.3037\n",
      "Epoch [89/500], Step [40/58], Loss: 0.4966\n",
      "Epoch [89/500], Step [50/58], Loss: 0.1993\n",
      "Epoch [90/500], Step [10/58], Loss: 0.2127\n",
      "Epoch [90/500], Step [20/58], Loss: 0.1836\n",
      "Epoch [90/500], Step [30/58], Loss: 0.4047\n",
      "Epoch [90/500], Step [40/58], Loss: 0.6366\n",
      "Epoch [90/500], Step [50/58], Loss: 0.4842\n",
      "Epoch [91/500], Step [10/58], Loss: 0.5057\n",
      "Epoch [91/500], Step [20/58], Loss: 0.8308\n",
      "Epoch [91/500], Step [30/58], Loss: 0.3316\n",
      "Epoch [91/500], Step [40/58], Loss: 0.5054\n",
      "Epoch [91/500], Step [50/58], Loss: 0.5779\n",
      "Epoch [92/500], Step [10/58], Loss: 0.3336\n",
      "Epoch [92/500], Step [20/58], Loss: 0.7905\n",
      "Epoch [92/500], Step [30/58], Loss: 0.2631\n",
      "Epoch [92/500], Step [40/58], Loss: 0.3826\n",
      "Epoch [92/500], Step [50/58], Loss: 0.9484\n",
      "Epoch [93/500], Step [10/58], Loss: 0.5650\n",
      "Epoch [93/500], Step [20/58], Loss: 0.5401\n",
      "Epoch [93/500], Step [30/58], Loss: 0.6109\n",
      "Epoch [93/500], Step [40/58], Loss: 0.7480\n",
      "Epoch [93/500], Step [50/58], Loss: 0.4482\n",
      "Epoch [94/500], Step [10/58], Loss: 0.8322\n",
      "Epoch [94/500], Step [20/58], Loss: 0.6937\n",
      "Epoch [94/500], Step [30/58], Loss: 0.4425\n",
      "Epoch [94/500], Step [40/58], Loss: 0.3998\n",
      "Epoch [94/500], Step [50/58], Loss: 0.5637\n",
      "Epoch [95/500], Step [10/58], Loss: 0.3470\n",
      "Epoch [95/500], Step [20/58], Loss: 0.3705\n",
      "Epoch [95/500], Step [30/58], Loss: 0.1894\n",
      "Epoch [95/500], Step [40/58], Loss: 0.3930\n",
      "Epoch [95/500], Step [50/58], Loss: 0.4364\n",
      "Epoch [96/500], Step [10/58], Loss: 0.4340\n",
      "Epoch [96/500], Step [20/58], Loss: 0.7763\n",
      "Epoch [96/500], Step [30/58], Loss: 0.2505\n",
      "Epoch [96/500], Step [40/58], Loss: 0.2255\n",
      "Epoch [96/500], Step [50/58], Loss: 0.4764\n",
      "Epoch [97/500], Step [10/58], Loss: 0.3621\n",
      "Epoch [97/500], Step [20/58], Loss: 0.4402\n",
      "Epoch [97/500], Step [30/58], Loss: 0.8055\n",
      "Epoch [97/500], Step [40/58], Loss: 0.5210\n",
      "Epoch [97/500], Step [50/58], Loss: 0.5555\n",
      "Epoch [98/500], Step [10/58], Loss: 0.5254\n",
      "Epoch [98/500], Step [20/58], Loss: 0.3475\n",
      "Epoch [98/500], Step [30/58], Loss: 0.2249\n",
      "Epoch [98/500], Step [40/58], Loss: 0.3771\n",
      "Epoch [98/500], Step [50/58], Loss: 0.2422\n",
      "Epoch [99/500], Step [10/58], Loss: 0.3813\n",
      "Epoch [99/500], Step [20/58], Loss: 0.2244\n",
      "Epoch [99/500], Step [30/58], Loss: 0.3050\n",
      "Epoch [99/500], Step [40/58], Loss: 0.2191\n",
      "Epoch [99/500], Step [50/58], Loss: 0.5349\n",
      "Epoch [100/500], Step [10/58], Loss: 0.4605\n",
      "Epoch [100/500], Step [20/58], Loss: 0.2302\n",
      "Epoch [100/500], Step [30/58], Loss: 0.4433\n",
      "Epoch [100/500], Step [40/58], Loss: 0.3677\n",
      "Epoch [100/500], Step [50/58], Loss: 0.2909\n",
      "Epoch [101/500], Step [10/58], Loss: 0.2686\n",
      "Epoch [101/500], Step [20/58], Loss: 0.2830\n",
      "Epoch [101/500], Step [30/58], Loss: 0.6604\n",
      "Epoch [101/500], Step [40/58], Loss: 0.4664\n",
      "Epoch [101/500], Step [50/58], Loss: 0.4552\n",
      "Epoch [102/500], Step [10/58], Loss: 0.4949\n",
      "Epoch [102/500], Step [20/58], Loss: 0.2770\n",
      "Epoch [102/500], Step [30/58], Loss: 0.4783\n",
      "Epoch [102/500], Step [40/58], Loss: 0.3113\n",
      "Epoch [102/500], Step [50/58], Loss: 0.4762\n",
      "Epoch [103/500], Step [10/58], Loss: 0.3709\n",
      "Epoch [103/500], Step [20/58], Loss: 0.3050\n",
      "Epoch [103/500], Step [30/58], Loss: 0.3471\n",
      "Epoch [103/500], Step [40/58], Loss: 0.2824\n",
      "Epoch [103/500], Step [50/58], Loss: 0.3677\n",
      "Epoch [104/500], Step [10/58], Loss: 0.3722\n",
      "Epoch [104/500], Step [20/58], Loss: 0.6871\n",
      "Epoch [104/500], Step [30/58], Loss: 0.8824\n",
      "Epoch [104/500], Step [40/58], Loss: 0.5612\n",
      "Epoch [104/500], Step [50/58], Loss: 0.5825\n",
      "Epoch [105/500], Step [10/58], Loss: 0.3226\n",
      "Epoch [105/500], Step [20/58], Loss: 0.2510\n",
      "Epoch [105/500], Step [30/58], Loss: 0.2997\n",
      "Epoch [105/500], Step [40/58], Loss: 0.2592\n",
      "Epoch [105/500], Step [50/58], Loss: 0.3144\n",
      "Epoch [106/500], Step [10/58], Loss: 0.4238\n",
      "Epoch [106/500], Step [20/58], Loss: 0.3882\n",
      "Epoch [106/500], Step [30/58], Loss: 0.3086\n",
      "Epoch [106/500], Step [40/58], Loss: 0.1929\n",
      "Epoch [106/500], Step [50/58], Loss: 0.3007\n",
      "Epoch [107/500], Step [10/58], Loss: 0.1862\n",
      "Epoch [107/500], Step [20/58], Loss: 0.4721\n",
      "Epoch [107/500], Step [30/58], Loss: 0.2918\n",
      "Epoch [107/500], Step [40/58], Loss: 0.2177\n",
      "Epoch [107/500], Step [50/58], Loss: 0.2057\n",
      "Epoch [108/500], Step [10/58], Loss: 0.3722\n",
      "Epoch [108/500], Step [20/58], Loss: 0.1683\n",
      "Epoch [108/500], Step [30/58], Loss: 0.3842\n",
      "Epoch [108/500], Step [40/58], Loss: 0.2201\n",
      "Epoch [108/500], Step [50/58], Loss: 0.1829\n",
      "Epoch [109/500], Step [10/58], Loss: 0.1779\n",
      "Epoch [109/500], Step [20/58], Loss: 0.3126\n",
      "Epoch [109/500], Step [30/58], Loss: 0.3230\n",
      "Epoch [109/500], Step [40/58], Loss: 0.3239\n",
      "Epoch [109/500], Step [50/58], Loss: 0.6164\n",
      "Epoch [110/500], Step [10/58], Loss: 0.3009\n",
      "Epoch [110/500], Step [20/58], Loss: 0.1641\n",
      "Epoch [110/500], Step [30/58], Loss: 0.0921\n",
      "Epoch [110/500], Step [40/58], Loss: 0.4113\n",
      "Epoch [110/500], Step [50/58], Loss: 0.2953\n",
      "Epoch [111/500], Step [10/58], Loss: 0.2512\n",
      "Epoch [111/500], Step [20/58], Loss: 0.3546\n",
      "Epoch [111/500], Step [30/58], Loss: 0.4012\n",
      "Epoch [111/500], Step [40/58], Loss: 0.1474\n",
      "Epoch [111/500], Step [50/58], Loss: 0.3119\n",
      "Epoch [112/500], Step [10/58], Loss: 0.3770\n",
      "Epoch [112/500], Step [20/58], Loss: 0.2985\n",
      "Epoch [112/500], Step [30/58], Loss: 0.2089\n",
      "Epoch [112/500], Step [40/58], Loss: 0.1197\n",
      "Epoch [112/500], Step [50/58], Loss: 0.3310\n",
      "Epoch [113/500], Step [10/58], Loss: 0.4701\n",
      "Epoch [113/500], Step [20/58], Loss: 0.1682\n",
      "Epoch [113/500], Step [30/58], Loss: 0.3102\n",
      "Epoch [113/500], Step [40/58], Loss: 0.3069\n",
      "Epoch [113/500], Step [50/58], Loss: 0.4322\n",
      "Epoch [114/500], Step [10/58], Loss: 0.1847\n",
      "Epoch [114/500], Step [20/58], Loss: 0.2756\n",
      "Epoch [114/500], Step [30/58], Loss: 0.1389\n",
      "Epoch [114/500], Step [40/58], Loss: 0.1446\n",
      "Epoch [114/500], Step [50/58], Loss: 0.3201\n",
      "Epoch [115/500], Step [10/58], Loss: 0.1652\n",
      "Epoch [115/500], Step [20/58], Loss: 0.3452\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [115/500], Step [30/58], Loss: 0.3249\n",
      "Epoch [115/500], Step [40/58], Loss: 0.2840\n",
      "Epoch [115/500], Step [50/58], Loss: 0.4171\n",
      "Epoch [116/500], Step [10/58], Loss: 0.2806\n",
      "Epoch [116/500], Step [20/58], Loss: 0.1107\n",
      "Epoch [116/500], Step [30/58], Loss: 0.4045\n",
      "Epoch [116/500], Step [40/58], Loss: 0.2975\n",
      "Epoch [116/500], Step [50/58], Loss: 0.2141\n",
      "Epoch [117/500], Step [10/58], Loss: 0.2551\n",
      "Epoch [117/500], Step [20/58], Loss: 0.1608\n",
      "Epoch [117/500], Step [30/58], Loss: 0.3459\n",
      "Epoch [117/500], Step [40/58], Loss: 0.2516\n",
      "Epoch [117/500], Step [50/58], Loss: 0.1523\n",
      "Epoch [118/500], Step [10/58], Loss: 0.1529\n",
      "Epoch [118/500], Step [20/58], Loss: 0.3140\n",
      "Epoch [118/500], Step [30/58], Loss: 0.2051\n",
      "Epoch [118/500], Step [40/58], Loss: 0.3294\n",
      "Epoch [118/500], Step [50/58], Loss: 0.5649\n",
      "Epoch [119/500], Step [10/58], Loss: 0.3792\n",
      "Epoch [119/500], Step [20/58], Loss: 0.4846\n",
      "Epoch [119/500], Step [30/58], Loss: 0.2662\n",
      "Epoch [119/500], Step [40/58], Loss: 0.0874\n",
      "Epoch [119/500], Step [50/58], Loss: 0.2349\n",
      "Epoch [120/500], Step [10/58], Loss: 0.2503\n",
      "Epoch [120/500], Step [20/58], Loss: 0.0953\n",
      "Epoch [120/500], Step [30/58], Loss: 0.1173\n",
      "Epoch [120/500], Step [40/58], Loss: 0.5384\n",
      "Epoch [120/500], Step [50/58], Loss: 0.2006\n",
      "Epoch [121/500], Step [10/58], Loss: 0.2063\n",
      "Epoch [121/500], Step [20/58], Loss: 0.1224\n",
      "Epoch [121/500], Step [30/58], Loss: 0.1727\n",
      "Epoch [121/500], Step [40/58], Loss: 0.1798\n",
      "Epoch [121/500], Step [50/58], Loss: 0.2784\n",
      "Epoch [122/500], Step [10/58], Loss: 0.6207\n",
      "Epoch [122/500], Step [20/58], Loss: 0.2570\n",
      "Epoch [122/500], Step [30/58], Loss: 0.2761\n",
      "Epoch [122/500], Step [40/58], Loss: 0.3198\n",
      "Epoch [122/500], Step [50/58], Loss: 0.2833\n",
      "Epoch [123/500], Step [10/58], Loss: 0.1941\n",
      "Epoch [123/500], Step [20/58], Loss: 0.2403\n",
      "Epoch [123/500], Step [30/58], Loss: 0.4628\n",
      "Epoch [123/500], Step [40/58], Loss: 0.2276\n",
      "Epoch [123/500], Step [50/58], Loss: 0.4721\n",
      "Epoch [124/500], Step [10/58], Loss: 0.2780\n",
      "Epoch [124/500], Step [20/58], Loss: 0.2964\n",
      "Epoch [124/500], Step [30/58], Loss: 0.2717\n",
      "Epoch [124/500], Step [40/58], Loss: 0.2467\n",
      "Epoch [124/500], Step [50/58], Loss: 0.1099\n",
      "Epoch [125/500], Step [10/58], Loss: 0.2151\n",
      "Epoch [125/500], Step [20/58], Loss: 0.1233\n",
      "Epoch [125/500], Step [30/58], Loss: 0.1518\n",
      "Epoch [125/500], Step [40/58], Loss: 0.1694\n",
      "Epoch [125/500], Step [50/58], Loss: 0.2032\n",
      "Epoch [126/500], Step [10/58], Loss: 0.1823\n",
      "Epoch [126/500], Step [20/58], Loss: 0.2637\n",
      "Epoch [126/500], Step [30/58], Loss: 0.2117\n",
      "Epoch [126/500], Step [40/58], Loss: 0.1410\n",
      "Epoch [126/500], Step [50/58], Loss: 0.2196\n",
      "Epoch [127/500], Step [10/58], Loss: 0.2388\n",
      "Epoch [127/500], Step [20/58], Loss: 0.2811\n",
      "Epoch [127/500], Step [30/58], Loss: 0.1352\n",
      "Epoch [127/500], Step [40/58], Loss: 0.1115\n",
      "Epoch [127/500], Step [50/58], Loss: 0.2642\n",
      "Epoch [128/500], Step [10/58], Loss: 0.1789\n",
      "Epoch [128/500], Step [20/58], Loss: 0.2118\n",
      "Epoch [128/500], Step [30/58], Loss: 0.1345\n",
      "Epoch [128/500], Step [40/58], Loss: 0.1717\n",
      "Epoch [128/500], Step [50/58], Loss: 0.1600\n",
      "Epoch [129/500], Step [10/58], Loss: 0.1963\n",
      "Epoch [129/500], Step [20/58], Loss: 0.0596\n",
      "Epoch [129/500], Step [30/58], Loss: 0.2912\n",
      "Epoch [129/500], Step [40/58], Loss: 0.1522\n",
      "Epoch [129/500], Step [50/58], Loss: 0.0569\n",
      "Epoch [130/500], Step [10/58], Loss: 0.1604\n",
      "Epoch [130/500], Step [20/58], Loss: 0.2602\n",
      "Epoch [130/500], Step [30/58], Loss: 0.1755\n",
      "Epoch [130/500], Step [40/58], Loss: 0.1077\n",
      "Epoch [130/500], Step [50/58], Loss: 0.2244\n",
      "Epoch [131/500], Step [10/58], Loss: 0.3024\n",
      "Epoch [131/500], Step [20/58], Loss: 0.2575\n",
      "Epoch [131/500], Step [30/58], Loss: 0.2190\n",
      "Epoch [131/500], Step [40/58], Loss: 0.0581\n",
      "Epoch [131/500], Step [50/58], Loss: 0.1180\n",
      "Epoch [132/500], Step [10/58], Loss: 0.0841\n",
      "Epoch [132/500], Step [20/58], Loss: 0.1147\n",
      "Epoch [132/500], Step [30/58], Loss: 0.1275\n",
      "Epoch [132/500], Step [40/58], Loss: 0.1426\n",
      "Epoch [132/500], Step [50/58], Loss: 0.1075\n",
      "Epoch [133/500], Step [10/58], Loss: 0.2766\n",
      "Epoch [133/500], Step [20/58], Loss: 0.6186\n",
      "Epoch [133/500], Step [30/58], Loss: 0.5978\n",
      "Epoch [133/500], Step [40/58], Loss: 0.1786\n",
      "Epoch [133/500], Step [50/58], Loss: 0.1370\n",
      "Epoch [134/500], Step [10/58], Loss: 0.3617\n",
      "Epoch [134/500], Step [20/58], Loss: 0.2735\n",
      "Epoch [134/500], Step [30/58], Loss: 0.3918\n",
      "Epoch [134/500], Step [40/58], Loss: 0.1565\n",
      "Epoch [134/500], Step [50/58], Loss: 0.1399\n",
      "Epoch [135/500], Step [10/58], Loss: 0.1197\n",
      "Epoch [135/500], Step [20/58], Loss: 0.0759\n",
      "Epoch [135/500], Step [30/58], Loss: 0.3491\n",
      "Epoch [135/500], Step [40/58], Loss: 0.0772\n",
      "Epoch [135/500], Step [50/58], Loss: 0.3015\n",
      "Epoch [136/500], Step [10/58], Loss: 0.1121\n",
      "Epoch [136/500], Step [20/58], Loss: 0.1024\n",
      "Epoch [136/500], Step [30/58], Loss: 0.2143\n",
      "Epoch [136/500], Step [40/58], Loss: 0.3003\n",
      "Epoch [136/500], Step [50/58], Loss: 0.1610\n",
      "Epoch [137/500], Step [10/58], Loss: 0.0475\n",
      "Epoch [137/500], Step [20/58], Loss: 0.1104\n",
      "Epoch [137/500], Step [30/58], Loss: 0.2485\n",
      "Epoch [137/500], Step [40/58], Loss: 0.1032\n",
      "Epoch [137/500], Step [50/58], Loss: 0.1436\n",
      "Epoch [138/500], Step [10/58], Loss: 0.1375\n",
      "Epoch [138/500], Step [20/58], Loss: 0.0533\n",
      "Epoch [138/500], Step [30/58], Loss: 0.2942\n",
      "Epoch [138/500], Step [40/58], Loss: 0.3121\n",
      "Epoch [138/500], Step [50/58], Loss: 0.1921\n",
      "Epoch [139/500], Step [10/58], Loss: 0.4833\n",
      "Epoch [139/500], Step [20/58], Loss: 0.3143\n",
      "Epoch [139/500], Step [30/58], Loss: 0.1542\n",
      "Epoch [139/500], Step [40/58], Loss: 0.4061\n",
      "Epoch [139/500], Step [50/58], Loss: 0.3471\n",
      "Epoch [140/500], Step [10/58], Loss: 0.2172\n",
      "Epoch [140/500], Step [20/58], Loss: 0.0719\n",
      "Epoch [140/500], Step [30/58], Loss: 0.3083\n",
      "Epoch [140/500], Step [40/58], Loss: 0.0904\n",
      "Epoch [140/500], Step [50/58], Loss: 0.1886\n",
      "Epoch [141/500], Step [10/58], Loss: 0.0326\n",
      "Epoch [141/500], Step [20/58], Loss: 0.0802\n",
      "Epoch [141/500], Step [30/58], Loss: 0.1581\n",
      "Epoch [141/500], Step [40/58], Loss: 0.2299\n",
      "Epoch [141/500], Step [50/58], Loss: 0.3181\n",
      "Epoch [142/500], Step [10/58], Loss: 0.1333\n",
      "Epoch [142/500], Step [20/58], Loss: 0.1136\n",
      "Epoch [142/500], Step [30/58], Loss: 0.1517\n",
      "Epoch [142/500], Step [40/58], Loss: 0.0847\n",
      "Epoch [142/500], Step [50/58], Loss: 0.1741\n",
      "Epoch [143/500], Step [10/58], Loss: 0.1384\n",
      "Epoch [143/500], Step [20/58], Loss: 0.1286\n",
      "Epoch [143/500], Step [30/58], Loss: 0.1710\n",
      "Epoch [143/500], Step [40/58], Loss: 0.1589\n",
      "Epoch [143/500], Step [50/58], Loss: 0.2114\n",
      "Epoch [144/500], Step [10/58], Loss: 0.0724\n",
      "Epoch [144/500], Step [20/58], Loss: 0.2734\n",
      "Epoch [144/500], Step [30/58], Loss: 0.2399\n",
      "Epoch [144/500], Step [40/58], Loss: 0.0809\n",
      "Epoch [144/500], Step [50/58], Loss: 0.0588\n",
      "Epoch [145/500], Step [10/58], Loss: 0.1785\n",
      "Epoch [145/500], Step [20/58], Loss: 0.0374\n",
      "Epoch [145/500], Step [30/58], Loss: 0.1481\n",
      "Epoch [145/500], Step [40/58], Loss: 0.0342\n",
      "Epoch [145/500], Step [50/58], Loss: 0.0744\n",
      "Epoch [146/500], Step [10/58], Loss: 0.0890\n",
      "Epoch [146/500], Step [20/58], Loss: 0.5538\n",
      "Epoch [146/500], Step [30/58], Loss: 0.0664\n",
      "Epoch [146/500], Step [40/58], Loss: 0.1349\n",
      "Epoch [146/500], Step [50/58], Loss: 0.0887\n",
      "Epoch [147/500], Step [10/58], Loss: 0.1020\n",
      "Epoch [147/500], Step [20/58], Loss: 0.0462\n",
      "Epoch [147/500], Step [30/58], Loss: 0.1264\n",
      "Epoch [147/500], Step [40/58], Loss: 0.0508\n",
      "Epoch [147/500], Step [50/58], Loss: 0.0802\n",
      "Epoch [148/500], Step [10/58], Loss: 0.2109\n",
      "Epoch [148/500], Step [20/58], Loss: 0.2823\n",
      "Epoch [148/500], Step [30/58], Loss: 0.1172\n",
      "Epoch [148/500], Step [40/58], Loss: 0.0845\n",
      "Epoch [148/500], Step [50/58], Loss: 0.1575\n",
      "Epoch [149/500], Step [10/58], Loss: 0.0213\n",
      "Epoch [149/500], Step [20/58], Loss: 0.0914\n",
      "Epoch [149/500], Step [30/58], Loss: 0.0514\n",
      "Epoch [149/500], Step [40/58], Loss: 0.4385\n",
      "Epoch [149/500], Step [50/58], Loss: 0.3474\n",
      "Epoch [150/500], Step [10/58], Loss: 0.2159\n",
      "Epoch [150/500], Step [20/58], Loss: 0.1481\n",
      "Epoch [150/500], Step [30/58], Loss: 0.1468\n",
      "Epoch [150/500], Step [40/58], Loss: 0.0984\n",
      "Epoch [150/500], Step [50/58], Loss: 0.1685\n",
      "Epoch [151/500], Step [10/58], Loss: 0.0530\n",
      "Epoch [151/500], Step [20/58], Loss: 0.2441\n",
      "Epoch [151/500], Step [30/58], Loss: 0.1308\n",
      "Epoch [151/500], Step [40/58], Loss: 0.1480\n",
      "Epoch [151/500], Step [50/58], Loss: 0.0932\n",
      "Epoch [152/500], Step [10/58], Loss: 0.0543\n",
      "Epoch [152/500], Step [20/58], Loss: 0.1189\n",
      "Epoch [152/500], Step [30/58], Loss: 0.3424\n",
      "Epoch [152/500], Step [40/58], Loss: 0.2324\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [152/500], Step [50/58], Loss: 0.1812\n",
      "Epoch [153/500], Step [10/58], Loss: 0.0217\n",
      "Epoch [153/500], Step [20/58], Loss: 0.0276\n",
      "Epoch [153/500], Step [30/58], Loss: 0.1267\n",
      "Epoch [153/500], Step [40/58], Loss: 0.0651\n",
      "Epoch [153/500], Step [50/58], Loss: 0.1212\n",
      "Epoch [154/500], Step [10/58], Loss: 0.1465\n",
      "Epoch [154/500], Step [20/58], Loss: 0.1196\n",
      "Epoch [154/500], Step [30/58], Loss: 0.0630\n",
      "Epoch [154/500], Step [40/58], Loss: 0.1803\n",
      "Epoch [154/500], Step [50/58], Loss: 0.0454\n",
      "Epoch [155/500], Step [10/58], Loss: 0.2937\n",
      "Epoch [155/500], Step [20/58], Loss: 0.3063\n",
      "Epoch [155/500], Step [30/58], Loss: 0.1820\n",
      "Epoch [155/500], Step [40/58], Loss: 0.0837\n",
      "Epoch [155/500], Step [50/58], Loss: 0.2315\n",
      "Epoch [156/500], Step [10/58], Loss: 0.0475\n",
      "Epoch [156/500], Step [20/58], Loss: 0.1684\n",
      "Epoch [156/500], Step [30/58], Loss: 0.4771\n",
      "Epoch [156/500], Step [40/58], Loss: 0.2636\n",
      "Epoch [156/500], Step [50/58], Loss: 0.6424\n",
      "Epoch [157/500], Step [10/58], Loss: 0.0531\n",
      "Epoch [157/500], Step [20/58], Loss: 0.3719\n",
      "Epoch [157/500], Step [30/58], Loss: 0.1456\n",
      "Epoch [157/500], Step [40/58], Loss: 0.2763\n",
      "Epoch [157/500], Step [50/58], Loss: 0.3972\n",
      "Epoch [158/500], Step [10/58], Loss: 0.3785\n",
      "Epoch [158/500], Step [20/58], Loss: 0.3061\n",
      "Epoch [158/500], Step [30/58], Loss: 0.3932\n",
      "Epoch [158/500], Step [40/58], Loss: 0.1515\n",
      "Epoch [158/500], Step [50/58], Loss: 0.3007\n",
      "Epoch [159/500], Step [10/58], Loss: 0.0652\n",
      "Epoch [159/500], Step [20/58], Loss: 0.3190\n",
      "Epoch [159/500], Step [30/58], Loss: 0.0865\n",
      "Epoch [159/500], Step [40/58], Loss: 0.0751\n",
      "Epoch [159/500], Step [50/58], Loss: 0.0704\n",
      "Epoch [160/500], Step [10/58], Loss: 0.0352\n",
      "Epoch [160/500], Step [20/58], Loss: 0.0373\n",
      "Epoch [160/500], Step [30/58], Loss: 0.0985\n",
      "Epoch [160/500], Step [40/58], Loss: 0.0316\n",
      "Epoch [160/500], Step [50/58], Loss: 0.0315\n",
      "Epoch [161/500], Step [10/58], Loss: 0.0406\n",
      "Epoch [161/500], Step [20/58], Loss: 0.0727\n",
      "Epoch [161/500], Step [30/58], Loss: 0.0964\n",
      "Epoch [161/500], Step [40/58], Loss: 0.1942\n",
      "Epoch [161/500], Step [50/58], Loss: 0.1644\n",
      "Epoch [162/500], Step [10/58], Loss: 0.1214\n",
      "Epoch [162/500], Step [20/58], Loss: 0.1448\n",
      "Epoch [162/500], Step [30/58], Loss: 0.0245\n",
      "Epoch [162/500], Step [40/58], Loss: 0.0694\n",
      "Epoch [162/500], Step [50/58], Loss: 0.0862\n",
      "Epoch [163/500], Step [10/58], Loss: 0.0136\n",
      "Epoch [163/500], Step [20/58], Loss: 0.0465\n",
      "Epoch [163/500], Step [30/58], Loss: 0.1187\n",
      "Epoch [163/500], Step [40/58], Loss: 0.1878\n",
      "Epoch [163/500], Step [50/58], Loss: 0.1521\n",
      "Epoch [164/500], Step [10/58], Loss: 0.0828\n",
      "Epoch [164/500], Step [20/58], Loss: 0.0148\n",
      "Epoch [164/500], Step [30/58], Loss: 0.0131\n",
      "Epoch [164/500], Step [40/58], Loss: 0.2412\n",
      "Epoch [164/500], Step [50/58], Loss: 0.2556\n",
      "Epoch [165/500], Step [10/58], Loss: 0.0413\n",
      "Epoch [165/500], Step [20/58], Loss: 0.1493\n",
      "Epoch [165/500], Step [30/58], Loss: 0.0820\n",
      "Epoch [165/500], Step [40/58], Loss: 0.3674\n",
      "Epoch [165/500], Step [50/58], Loss: 0.0688\n",
      "Epoch [166/500], Step [10/58], Loss: 0.4628\n",
      "Epoch [166/500], Step [20/58], Loss: 0.2832\n",
      "Epoch [166/500], Step [30/58], Loss: 0.0480\n",
      "Epoch [166/500], Step [40/58], Loss: 0.0983\n",
      "Epoch [166/500], Step [50/58], Loss: 0.1191\n",
      "Epoch [167/500], Step [10/58], Loss: 0.2173\n",
      "Epoch [167/500], Step [20/58], Loss: 0.1809\n",
      "Epoch [167/500], Step [30/58], Loss: 0.6543\n",
      "Epoch [167/500], Step [40/58], Loss: 1.2242\n",
      "Epoch [167/500], Step [50/58], Loss: 2.6701\n",
      "Epoch [168/500], Step [10/58], Loss: 1.6113\n",
      "Epoch [168/500], Step [20/58], Loss: 1.5559\n",
      "Epoch [168/500], Step [30/58], Loss: 0.8085\n",
      "Epoch [168/500], Step [40/58], Loss: 0.5616\n",
      "Epoch [168/500], Step [50/58], Loss: 0.7000\n",
      "Epoch [169/500], Step [10/58], Loss: 0.2251\n",
      "Epoch [169/500], Step [20/58], Loss: 0.6192\n",
      "Epoch [169/500], Step [30/58], Loss: 0.3527\n",
      "Epoch [169/500], Step [40/58], Loss: 0.3741\n",
      "Epoch [169/500], Step [50/58], Loss: 0.4007\n",
      "Epoch [170/500], Step [10/58], Loss: 0.1559\n",
      "Epoch [170/500], Step [20/58], Loss: 0.1142\n",
      "Epoch [170/500], Step [30/58], Loss: 0.0942\n",
      "Epoch [170/500], Step [40/58], Loss: 0.1925\n",
      "Epoch [170/500], Step [50/58], Loss: 0.1145\n",
      "Epoch [171/500], Step [10/58], Loss: 0.8235\n",
      "Epoch [171/500], Step [20/58], Loss: 0.5988\n",
      "Epoch [171/500], Step [30/58], Loss: 0.4174\n",
      "Epoch [171/500], Step [40/58], Loss: 0.7576\n",
      "Epoch [171/500], Step [50/58], Loss: 0.3071\n",
      "Epoch [172/500], Step [10/58], Loss: 0.2936\n",
      "Epoch [172/500], Step [20/58], Loss: 0.4592\n",
      "Epoch [172/500], Step [30/58], Loss: 0.3350\n",
      "Epoch [172/500], Step [40/58], Loss: 0.6074\n",
      "Epoch [172/500], Step [50/58], Loss: 0.2812\n",
      "Epoch [173/500], Step [10/58], Loss: 0.3104\n",
      "Epoch [173/500], Step [20/58], Loss: 0.3302\n",
      "Epoch [173/500], Step [30/58], Loss: 0.2178\n",
      "Epoch [173/500], Step [40/58], Loss: 0.1428\n",
      "Epoch [173/500], Step [50/58], Loss: 0.1656\n",
      "Epoch [174/500], Step [10/58], Loss: 0.2306\n",
      "Epoch [174/500], Step [20/58], Loss: 0.2513\n",
      "Epoch [174/500], Step [30/58], Loss: 0.2618\n",
      "Epoch [174/500], Step [40/58], Loss: 0.1422\n",
      "Epoch [174/500], Step [50/58], Loss: 0.2821\n",
      "Epoch [175/500], Step [10/58], Loss: 0.0663\n",
      "Epoch [175/500], Step [20/58], Loss: 0.1036\n",
      "Epoch [175/500], Step [30/58], Loss: 0.2074\n",
      "Epoch [175/500], Step [40/58], Loss: 0.1728\n",
      "Epoch [175/500], Step [50/58], Loss: 0.2497\n",
      "Epoch [176/500], Step [10/58], Loss: 0.1319\n",
      "Epoch [176/500], Step [20/58], Loss: 0.0726\n",
      "Epoch [176/500], Step [30/58], Loss: 0.3834\n",
      "Epoch [176/500], Step [40/58], Loss: 0.1459\n",
      "Epoch [176/500], Step [50/58], Loss: 0.1420\n",
      "Epoch [177/500], Step [10/58], Loss: 0.2914\n",
      "Epoch [177/500], Step [20/58], Loss: 0.1031\n",
      "Epoch [177/500], Step [30/58], Loss: 0.1131\n",
      "Epoch [177/500], Step [40/58], Loss: 0.2423\n",
      "Epoch [177/500], Step [50/58], Loss: 0.3975\n",
      "Epoch [178/500], Step [10/58], Loss: 0.2728\n",
      "Epoch [178/500], Step [20/58], Loss: 0.0943\n",
      "Epoch [178/500], Step [30/58], Loss: 0.1990\n",
      "Epoch [178/500], Step [40/58], Loss: 0.2134\n",
      "Epoch [178/500], Step [50/58], Loss: 0.8689\n",
      "Epoch [179/500], Step [10/58], Loss: 0.3600\n",
      "Epoch [179/500], Step [20/58], Loss: 0.1082\n",
      "Epoch [179/500], Step [30/58], Loss: 0.1026\n",
      "Epoch [179/500], Step [40/58], Loss: 0.0688\n",
      "Epoch [179/500], Step [50/58], Loss: 0.1840\n",
      "Epoch [180/500], Step [10/58], Loss: 0.1599\n",
      "Epoch [180/500], Step [20/58], Loss: 0.1074\n",
      "Epoch [180/500], Step [30/58], Loss: 0.0631\n",
      "Epoch [180/500], Step [40/58], Loss: 0.1121\n",
      "Epoch [180/500], Step [50/58], Loss: 0.2196\n",
      "Epoch [181/500], Step [10/58], Loss: 0.0844\n",
      "Epoch [181/500], Step [20/58], Loss: 0.2273\n",
      "Epoch [181/500], Step [30/58], Loss: 0.0900\n",
      "Epoch [181/500], Step [40/58], Loss: 0.2414\n",
      "Epoch [181/500], Step [50/58], Loss: 0.0489\n",
      "Epoch [182/500], Step [10/58], Loss: 0.0529\n",
      "Epoch [182/500], Step [20/58], Loss: 0.0151\n",
      "Epoch [182/500], Step [30/58], Loss: 0.0715\n",
      "Epoch [182/500], Step [40/58], Loss: 0.0726\n",
      "Epoch [182/500], Step [50/58], Loss: 0.0314\n",
      "Epoch [183/500], Step [10/58], Loss: 0.0117\n",
      "Epoch [183/500], Step [20/58], Loss: 0.0283\n",
      "Epoch [183/500], Step [30/58], Loss: 0.0521\n",
      "Epoch [183/500], Step [40/58], Loss: 0.0415\n",
      "Epoch [183/500], Step [50/58], Loss: 0.0416\n",
      "Epoch [184/500], Step [10/58], Loss: 0.2228\n",
      "Epoch [184/500], Step [20/58], Loss: 0.0500\n",
      "Epoch [184/500], Step [30/58], Loss: 0.2491\n",
      "Epoch [184/500], Step [40/58], Loss: 0.1274\n",
      "Epoch [184/500], Step [50/58], Loss: 0.2878\n",
      "Epoch [185/500], Step [10/58], Loss: 0.0506\n",
      "Epoch [185/500], Step [20/58], Loss: 0.0575\n",
      "Epoch [185/500], Step [30/58], Loss: 0.0406\n",
      "Epoch [185/500], Step [40/58], Loss: 0.0844\n",
      "Epoch [185/500], Step [50/58], Loss: 0.0879\n",
      "Epoch [186/500], Step [10/58], Loss: 0.0538\n",
      "Epoch [186/500], Step [20/58], Loss: 0.0466\n",
      "Epoch [186/500], Step [30/58], Loss: 0.0432\n",
      "Epoch [186/500], Step [40/58], Loss: 0.0818\n",
      "Epoch [186/500], Step [50/58], Loss: 0.0793\n",
      "Epoch [187/500], Step [10/58], Loss: 0.0143\n",
      "Epoch [187/500], Step [20/58], Loss: 0.0248\n",
      "Epoch [187/500], Step [30/58], Loss: 0.0727\n",
      "Epoch [187/500], Step [40/58], Loss: 0.0164\n",
      "Epoch [187/500], Step [50/58], Loss: 0.2848\n",
      "Epoch [188/500], Step [10/58], Loss: 0.4275\n",
      "Epoch [188/500], Step [20/58], Loss: 0.3395\n",
      "Epoch [188/500], Step [30/58], Loss: 0.2227\n",
      "Epoch [188/500], Step [40/58], Loss: 0.0435\n",
      "Epoch [188/500], Step [50/58], Loss: 0.2392\n",
      "Epoch [189/500], Step [10/58], Loss: 0.5343\n",
      "Epoch [189/500], Step [20/58], Loss: 0.0905\n",
      "Epoch [189/500], Step [30/58], Loss: 0.1206\n",
      "Epoch [189/500], Step [40/58], Loss: 0.0699\n",
      "Epoch [189/500], Step [50/58], Loss: 0.0700\n",
      "Epoch [190/500], Step [10/58], Loss: 0.0808\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [190/500], Step [20/58], Loss: 0.0746\n",
      "Epoch [190/500], Step [30/58], Loss: 0.0290\n",
      "Epoch [190/500], Step [40/58], Loss: 0.0662\n",
      "Epoch [190/500], Step [50/58], Loss: 0.2910\n",
      "Epoch [191/500], Step [10/58], Loss: 0.1332\n",
      "Epoch [191/500], Step [20/58], Loss: 0.1498\n",
      "Epoch [191/500], Step [30/58], Loss: 0.1828\n",
      "Epoch [191/500], Step [40/58], Loss: 0.0841\n",
      "Epoch [191/500], Step [50/58], Loss: 0.0816\n",
      "Epoch [192/500], Step [10/58], Loss: 0.4619\n",
      "Epoch [192/500], Step [20/58], Loss: 0.0890\n",
      "Epoch [192/500], Step [30/58], Loss: 0.0915\n",
      "Epoch [192/500], Step [40/58], Loss: 0.0571\n",
      "Epoch [192/500], Step [50/58], Loss: 0.1049\n",
      "Epoch [193/500], Step [10/58], Loss: 0.2770\n",
      "Epoch [193/500], Step [20/58], Loss: 0.0539\n",
      "Epoch [193/500], Step [30/58], Loss: 0.0514\n",
      "Epoch [193/500], Step [40/58], Loss: 0.0785\n",
      "Epoch [193/500], Step [50/58], Loss: 0.1926\n",
      "Epoch [194/500], Step [10/58], Loss: 0.1117\n",
      "Epoch [194/500], Step [20/58], Loss: 0.0688\n",
      "Epoch [194/500], Step [30/58], Loss: 0.0557\n",
      "Epoch [194/500], Step [40/58], Loss: 0.0216\n",
      "Epoch [194/500], Step [50/58], Loss: 0.0198\n",
      "Epoch [195/500], Step [10/58], Loss: 0.1466\n",
      "Epoch [195/500], Step [20/58], Loss: 0.1340\n",
      "Epoch [195/500], Step [30/58], Loss: 0.0319\n",
      "Epoch [195/500], Step [40/58], Loss: 0.0169\n",
      "Epoch [195/500], Step [50/58], Loss: 0.0128\n",
      "Epoch [196/500], Step [10/58], Loss: 0.0275\n",
      "Epoch [196/500], Step [20/58], Loss: 0.0745\n",
      "Epoch [196/500], Step [30/58], Loss: 0.1499\n",
      "Epoch [196/500], Step [40/58], Loss: 0.0572\n",
      "Epoch [196/500], Step [50/58], Loss: 0.0730\n",
      "Epoch [197/500], Step [10/58], Loss: 0.0305\n",
      "Epoch [197/500], Step [20/58], Loss: 0.0426\n",
      "Epoch [197/500], Step [30/58], Loss: 0.0336\n",
      "Epoch [197/500], Step [40/58], Loss: 0.0329\n",
      "Epoch [197/500], Step [50/58], Loss: 0.1047\n",
      "Epoch [198/500], Step [10/58], Loss: 0.0550\n",
      "Epoch [198/500], Step [20/58], Loss: 0.0294\n",
      "Epoch [198/500], Step [30/58], Loss: 0.0440\n",
      "Epoch [198/500], Step [40/58], Loss: 0.3768\n",
      "Epoch [198/500], Step [50/58], Loss: 0.3847\n",
      "Epoch [199/500], Step [10/58], Loss: 0.1284\n",
      "Epoch [199/500], Step [20/58], Loss: 0.2988\n",
      "Epoch [199/500], Step [30/58], Loss: 0.2993\n",
      "Epoch [199/500], Step [40/58], Loss: 0.0887\n",
      "Epoch [199/500], Step [50/58], Loss: 0.2524\n",
      "Epoch [200/500], Step [10/58], Loss: 0.0591\n",
      "Epoch [200/500], Step [20/58], Loss: 0.0763\n",
      "Epoch [200/500], Step [30/58], Loss: 0.0930\n",
      "Epoch [200/500], Step [40/58], Loss: 0.0565\n",
      "Epoch [200/500], Step [50/58], Loss: 0.0747\n",
      "Epoch [201/500], Step [10/58], Loss: 0.0279\n",
      "Epoch [201/500], Step [20/58], Loss: 0.0257\n",
      "Epoch [201/500], Step [30/58], Loss: 0.1185\n",
      "Epoch [201/500], Step [40/58], Loss: 0.0712\n",
      "Epoch [201/500], Step [50/58], Loss: 0.1165\n",
      "Epoch [202/500], Step [10/58], Loss: 0.0709\n",
      "Epoch [202/500], Step [20/58], Loss: 0.1789\n",
      "Epoch [202/500], Step [30/58], Loss: 0.0893\n",
      "Epoch [202/500], Step [40/58], Loss: 0.2273\n",
      "Epoch [202/500], Step [50/58], Loss: 0.1966\n",
      "Epoch [203/500], Step [10/58], Loss: 0.0170\n",
      "Epoch [203/500], Step [20/58], Loss: 0.0874\n",
      "Epoch [203/500], Step [30/58], Loss: 0.0761\n",
      "Epoch [203/500], Step [40/58], Loss: 0.0271\n",
      "Epoch [203/500], Step [50/58], Loss: 0.0173\n",
      "Epoch [204/500], Step [10/58], Loss: 0.1783\n",
      "Epoch [204/500], Step [20/58], Loss: 0.1120\n",
      "Epoch [204/500], Step [30/58], Loss: 0.0461\n",
      "Epoch [204/500], Step [40/58], Loss: 0.2252\n",
      "Epoch [204/500], Step [50/58], Loss: 0.0490\n",
      "Epoch [205/500], Step [10/58], Loss: 0.0723\n",
      "Epoch [205/500], Step [20/58], Loss: 0.0071\n",
      "Epoch [205/500], Step [30/58], Loss: 0.2383\n",
      "Epoch [205/500], Step [40/58], Loss: 0.0376\n",
      "Epoch [205/500], Step [50/58], Loss: 0.0636\n",
      "Epoch [206/500], Step [10/58], Loss: 0.0933\n",
      "Epoch [206/500], Step [20/58], Loss: 0.0129\n",
      "Epoch [206/500], Step [30/58], Loss: 0.0211\n",
      "Epoch [206/500], Step [40/58], Loss: 0.0287\n",
      "Epoch [206/500], Step [50/58], Loss: 0.0113\n",
      "Epoch [207/500], Step [10/58], Loss: 0.1481\n",
      "Epoch [207/500], Step [20/58], Loss: 0.0394\n",
      "Epoch [207/500], Step [30/58], Loss: 0.0700\n",
      "Epoch [207/500], Step [40/58], Loss: 0.0666\n",
      "Epoch [207/500], Step [50/58], Loss: 0.0653\n",
      "Epoch [208/500], Step [10/58], Loss: 0.1141\n",
      "Epoch [208/500], Step [20/58], Loss: 0.0276\n",
      "Epoch [208/500], Step [30/58], Loss: 0.0343\n",
      "Epoch [208/500], Step [40/58], Loss: 0.0160\n",
      "Epoch [208/500], Step [50/58], Loss: 0.0305\n",
      "Epoch [209/500], Step [10/58], Loss: 0.0329\n",
      "Epoch [209/500], Step [20/58], Loss: 0.1516\n",
      "Epoch [209/500], Step [30/58], Loss: 0.0541\n",
      "Epoch [209/500], Step [40/58], Loss: 0.4033\n",
      "Epoch [209/500], Step [50/58], Loss: 0.1853\n",
      "Epoch [210/500], Step [10/58], Loss: 0.1763\n",
      "Epoch [210/500], Step [20/58], Loss: 0.1016\n",
      "Epoch [210/500], Step [30/58], Loss: 0.0508\n",
      "Epoch [210/500], Step [40/58], Loss: 0.0277\n",
      "Epoch [210/500], Step [50/58], Loss: 0.1054\n",
      "Epoch [211/500], Step [10/58], Loss: 0.0473\n",
      "Epoch [211/500], Step [20/58], Loss: 0.0548\n",
      "Epoch [211/500], Step [30/58], Loss: 0.0191\n",
      "Epoch [211/500], Step [40/58], Loss: 0.0781\n",
      "Epoch [211/500], Step [50/58], Loss: 0.0573\n",
      "Epoch [212/500], Step [10/58], Loss: 0.0489\n",
      "Epoch [212/500], Step [20/58], Loss: 0.0240\n",
      "Epoch [212/500], Step [30/58], Loss: 0.0081\n",
      "Epoch [212/500], Step [40/58], Loss: 0.0184\n",
      "Epoch [212/500], Step [50/58], Loss: 0.0342\n",
      "Epoch [213/500], Step [10/58], Loss: 0.0987\n",
      "Epoch [213/500], Step [20/58], Loss: 0.0336\n",
      "Epoch [213/500], Step [30/58], Loss: 0.0698\n",
      "Epoch [213/500], Step [40/58], Loss: 0.0457\n",
      "Epoch [213/500], Step [50/58], Loss: 0.2151\n",
      "Epoch [214/500], Step [10/58], Loss: 0.0123\n",
      "Epoch [214/500], Step [20/58], Loss: 0.1040\n",
      "Epoch [214/500], Step [30/58], Loss: 0.2555\n",
      "Epoch [214/500], Step [40/58], Loss: 0.0466\n",
      "Epoch [214/500], Step [50/58], Loss: 0.0794\n",
      "Epoch [215/500], Step [10/58], Loss: 0.0526\n",
      "Epoch [215/500], Step [20/58], Loss: 0.0140\n",
      "Epoch [215/500], Step [30/58], Loss: 0.0605\n",
      "Epoch [215/500], Step [40/58], Loss: 0.0826\n",
      "Epoch [215/500], Step [50/58], Loss: 0.3674\n",
      "Epoch [216/500], Step [10/58], Loss: 0.5485\n",
      "Epoch [216/500], Step [20/58], Loss: 0.1204\n",
      "Epoch [216/500], Step [30/58], Loss: 0.1880\n",
      "Epoch [216/500], Step [40/58], Loss: 0.0413\n",
      "Epoch [216/500], Step [50/58], Loss: 0.1387\n",
      "Epoch [217/500], Step [10/58], Loss: 0.1076\n",
      "Epoch [217/500], Step [20/58], Loss: 0.0578\n",
      "Epoch [217/500], Step [30/58], Loss: 0.0414\n",
      "Epoch [217/500], Step [40/58], Loss: 0.0201\n",
      "Epoch [217/500], Step [50/58], Loss: 0.0106\n",
      "Epoch [218/500], Step [10/58], Loss: 0.1841\n",
      "Epoch [218/500], Step [20/58], Loss: 0.0737\n",
      "Epoch [218/500], Step [30/58], Loss: 0.0423\n",
      "Epoch [218/500], Step [40/58], Loss: 0.0102\n",
      "Epoch [218/500], Step [50/58], Loss: 0.0356\n",
      "Epoch [219/500], Step [10/58], Loss: 0.0257\n",
      "Epoch [219/500], Step [20/58], Loss: 0.0232\n",
      "Epoch [219/500], Step [30/58], Loss: 0.0106\n",
      "Epoch [219/500], Step [40/58], Loss: 0.0159\n",
      "Epoch [219/500], Step [50/58], Loss: 0.0185\n",
      "Epoch [220/500], Step [10/58], Loss: 0.0434\n",
      "Epoch [220/500], Step [20/58], Loss: 0.0546\n",
      "Epoch [220/500], Step [30/58], Loss: 0.0260\n",
      "Epoch [220/500], Step [40/58], Loss: 0.0314\n",
      "Epoch [220/500], Step [50/58], Loss: 0.1771\n",
      "Epoch [221/500], Step [10/58], Loss: 0.1489\n",
      "Epoch [221/500], Step [20/58], Loss: 0.1139\n",
      "Epoch [221/500], Step [30/58], Loss: 0.3583\n",
      "Epoch [221/500], Step [40/58], Loss: 0.3471\n",
      "Epoch [221/500], Step [50/58], Loss: 0.2901\n",
      "Epoch [222/500], Step [10/58], Loss: 0.4188\n",
      "Epoch [222/500], Step [20/58], Loss: 0.1937\n",
      "Epoch [222/500], Step [30/58], Loss: 0.0694\n",
      "Epoch [222/500], Step [40/58], Loss: 0.0550\n",
      "Epoch [222/500], Step [50/58], Loss: 0.0539\n",
      "Epoch [223/500], Step [10/58], Loss: 0.0236\n",
      "Epoch [223/500], Step [20/58], Loss: 0.2268\n",
      "Epoch [223/500], Step [30/58], Loss: 0.1080\n",
      "Epoch [223/500], Step [40/58], Loss: 0.0235\n",
      "Epoch [223/500], Step [50/58], Loss: 0.4413\n",
      "Epoch [224/500], Step [10/58], Loss: 0.4056\n",
      "Epoch [224/500], Step [20/58], Loss: 0.0781\n",
      "Epoch [224/500], Step [30/58], Loss: 0.7471\n",
      "Epoch [224/500], Step [40/58], Loss: 0.2546\n",
      "Epoch [224/500], Step [50/58], Loss: 0.3528\n",
      "Epoch [225/500], Step [10/58], Loss: 0.0148\n",
      "Epoch [225/500], Step [20/58], Loss: 0.4634\n",
      "Epoch [225/500], Step [30/58], Loss: 0.1270\n",
      "Epoch [225/500], Step [40/58], Loss: 0.0910\n",
      "Epoch [225/500], Step [50/58], Loss: 0.1100\n",
      "Epoch [226/500], Step [10/58], Loss: 0.0212\n",
      "Epoch [226/500], Step [20/58], Loss: 0.0929\n",
      "Epoch [226/500], Step [30/58], Loss: 0.0888\n",
      "Epoch [226/500], Step [40/58], Loss: 0.0556\n",
      "Epoch [226/500], Step [50/58], Loss: 0.0283\n",
      "Epoch [227/500], Step [10/58], Loss: 0.0386\n",
      "Epoch [227/500], Step [20/58], Loss: 0.0619\n",
      "Epoch [227/500], Step [30/58], Loss: 0.0436\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [227/500], Step [40/58], Loss: 0.0740\n",
      "Epoch [227/500], Step [50/58], Loss: 0.0346\n",
      "Epoch [228/500], Step [10/58], Loss: 0.0413\n",
      "Epoch [228/500], Step [20/58], Loss: 0.0306\n",
      "Epoch [228/500], Step [30/58], Loss: 0.0853\n",
      "Epoch [228/500], Step [40/58], Loss: 0.0821\n",
      "Epoch [228/500], Step [50/58], Loss: 0.0165\n",
      "Epoch [229/500], Step [10/58], Loss: 0.0232\n",
      "Epoch [229/500], Step [20/58], Loss: 0.0305\n",
      "Epoch [229/500], Step [30/58], Loss: 0.0247\n",
      "Epoch [229/500], Step [40/58], Loss: 0.0134\n",
      "Epoch [229/500], Step [50/58], Loss: 0.2358\n",
      "Epoch [230/500], Step [10/58], Loss: 0.0514\n",
      "Epoch [230/500], Step [20/58], Loss: 0.0479\n",
      "Epoch [230/500], Step [30/58], Loss: 0.0824\n",
      "Epoch [230/500], Step [40/58], Loss: 0.0291\n",
      "Epoch [230/500], Step [50/58], Loss: 0.0316\n",
      "Epoch [231/500], Step [10/58], Loss: 0.0460\n",
      "Epoch [231/500], Step [20/58], Loss: 0.0104\n",
      "Epoch [231/500], Step [30/58], Loss: 0.0295\n",
      "Epoch [231/500], Step [40/58], Loss: 0.0832\n",
      "Epoch [231/500], Step [50/58], Loss: 0.0187\n",
      "Epoch [232/500], Step [10/58], Loss: 0.0514\n",
      "Epoch [232/500], Step [20/58], Loss: 0.0218\n",
      "Epoch [232/500], Step [30/58], Loss: 0.0229\n",
      "Epoch [232/500], Step [40/58], Loss: 0.0159\n",
      "Epoch [232/500], Step [50/58], Loss: 0.3166\n",
      "Epoch [233/500], Step [10/58], Loss: 0.0055\n",
      "Epoch [233/500], Step [20/58], Loss: 0.1473\n",
      "Epoch [233/500], Step [30/58], Loss: 0.1168\n",
      "Epoch [233/500], Step [40/58], Loss: 0.0646\n",
      "Epoch [233/500], Step [50/58], Loss: 0.0446\n",
      "Epoch [234/500], Step [10/58], Loss: 0.0583\n",
      "Epoch [234/500], Step [20/58], Loss: 0.0173\n",
      "Epoch [234/500], Step [30/58], Loss: 0.0121\n",
      "Epoch [234/500], Step [40/58], Loss: 0.0260\n",
      "Epoch [234/500], Step [50/58], Loss: 0.0252\n",
      "Epoch [235/500], Step [10/58], Loss: 0.0059\n",
      "Epoch [235/500], Step [20/58], Loss: 0.0467\n",
      "Epoch [235/500], Step [30/58], Loss: 0.0216\n",
      "Epoch [235/500], Step [40/58], Loss: 0.0512\n",
      "Epoch [235/500], Step [50/58], Loss: 0.0109\n",
      "Epoch [236/500], Step [10/58], Loss: 0.2046\n",
      "Epoch [236/500], Step [20/58], Loss: 0.0659\n",
      "Epoch [236/500], Step [30/58], Loss: 0.1191\n",
      "Epoch [236/500], Step [40/58], Loss: 0.0193\n",
      "Epoch [236/500], Step [50/58], Loss: 0.0252\n",
      "Epoch [237/500], Step [10/58], Loss: 0.1253\n",
      "Epoch [237/500], Step [20/58], Loss: 0.5814\n",
      "Epoch [237/500], Step [30/58], Loss: 0.2407\n",
      "Epoch [237/500], Step [40/58], Loss: 0.2258\n",
      "Epoch [237/500], Step [50/58], Loss: 0.1637\n",
      "Epoch [238/500], Step [10/58], Loss: 0.0183\n",
      "Epoch [238/500], Step [20/58], Loss: 0.0487\n",
      "Epoch [238/500], Step [30/58], Loss: 0.0798\n",
      "Epoch [238/500], Step [40/58], Loss: 0.0469\n",
      "Epoch [238/500], Step [50/58], Loss: 0.1986\n",
      "Epoch [239/500], Step [10/58], Loss: 0.0791\n",
      "Epoch [239/500], Step [20/58], Loss: 0.0181\n",
      "Epoch [239/500], Step [30/58], Loss: 0.0802\n",
      "Epoch [239/500], Step [40/58], Loss: 0.0953\n",
      "Epoch [239/500], Step [50/58], Loss: 0.0212\n",
      "Epoch [240/500], Step [10/58], Loss: 0.2910\n",
      "Epoch [240/500], Step [20/58], Loss: 0.2206\n",
      "Epoch [240/500], Step [30/58], Loss: 0.1190\n",
      "Epoch [240/500], Step [40/58], Loss: 0.0940\n",
      "Epoch [240/500], Step [50/58], Loss: 0.0703\n",
      "Epoch [241/500], Step [10/58], Loss: 0.0195\n",
      "Epoch [241/500], Step [20/58], Loss: 0.0302\n",
      "Epoch [241/500], Step [30/58], Loss: 0.0240\n",
      "Epoch [241/500], Step [40/58], Loss: 0.1122\n",
      "Epoch [241/500], Step [50/58], Loss: 0.0203\n",
      "Epoch [242/500], Step [10/58], Loss: 0.1245\n",
      "Epoch [242/500], Step [20/58], Loss: 0.0312\n",
      "Epoch [242/500], Step [30/58], Loss: 0.0470\n",
      "Epoch [242/500], Step [40/58], Loss: 0.0569\n",
      "Epoch [242/500], Step [50/58], Loss: 0.1096\n",
      "Epoch [243/500], Step [10/58], Loss: 0.0512\n",
      "Epoch [243/500], Step [20/58], Loss: 0.0744\n",
      "Epoch [243/500], Step [30/58], Loss: 0.0511\n",
      "Epoch [243/500], Step [40/58], Loss: 0.0640\n",
      "Epoch [243/500], Step [50/58], Loss: 0.0068\n",
      "Epoch [244/500], Step [10/58], Loss: 0.0701\n",
      "Epoch [244/500], Step [20/58], Loss: 0.0415\n",
      "Epoch [244/500], Step [30/58], Loss: 0.0042\n",
      "Epoch [244/500], Step [40/58], Loss: 0.0105\n",
      "Epoch [244/500], Step [50/58], Loss: 0.0165\n",
      "Epoch [245/500], Step [10/58], Loss: 0.0249\n",
      "Epoch [245/500], Step [20/58], Loss: 0.0252\n",
      "Epoch [245/500], Step [30/58], Loss: 0.0159\n",
      "Epoch [245/500], Step [40/58], Loss: 0.0254\n",
      "Epoch [245/500], Step [50/58], Loss: 0.0240\n",
      "Epoch [246/500], Step [10/58], Loss: 0.0483\n",
      "Epoch [246/500], Step [20/58], Loss: 0.0419\n",
      "Epoch [246/500], Step [30/58], Loss: 0.0094\n",
      "Epoch [246/500], Step [40/58], Loss: 0.0212\n",
      "Epoch [246/500], Step [50/58], Loss: 0.0182\n",
      "Epoch [247/500], Step [10/58], Loss: 0.0066\n",
      "Epoch [247/500], Step [20/58], Loss: 0.0031\n",
      "Epoch [247/500], Step [30/58], Loss: 0.0045\n",
      "Epoch [247/500], Step [40/58], Loss: 0.0028\n",
      "Epoch [247/500], Step [50/58], Loss: 0.0044\n",
      "Epoch [248/500], Step [10/58], Loss: 0.0084\n",
      "Epoch [248/500], Step [20/58], Loss: 0.0100\n",
      "Epoch [248/500], Step [30/58], Loss: 0.0020\n",
      "Epoch [248/500], Step [40/58], Loss: 0.0018\n",
      "Epoch [248/500], Step [50/58], Loss: 0.0141\n",
      "Epoch [249/500], Step [10/58], Loss: 0.0080\n",
      "Epoch [249/500], Step [20/58], Loss: 0.0055\n",
      "Epoch [249/500], Step [30/58], Loss: 0.0015\n",
      "Epoch [249/500], Step [40/58], Loss: 0.0070\n",
      "Epoch [249/500], Step [50/58], Loss: 0.0065\n",
      "Epoch [250/500], Step [10/58], Loss: 0.0012\n",
      "Epoch [250/500], Step [20/58], Loss: 0.0043\n",
      "Epoch [250/500], Step [30/58], Loss: 0.0035\n",
      "Epoch [250/500], Step [40/58], Loss: 0.0041\n",
      "Epoch [250/500], Step [50/58], Loss: 0.0084\n",
      "Epoch [251/500], Step [10/58], Loss: 0.0028\n",
      "Epoch [251/500], Step [20/58], Loss: 0.0029\n",
      "Epoch [251/500], Step [30/58], Loss: 0.0012\n",
      "Epoch [251/500], Step [40/58], Loss: 0.0051\n",
      "Epoch [251/500], Step [50/58], Loss: 0.0013\n",
      "Epoch [252/500], Step [10/58], Loss: 0.0065\n",
      "Epoch [252/500], Step [20/58], Loss: 0.0028\n",
      "Epoch [252/500], Step [30/58], Loss: 0.0027\n",
      "Epoch [252/500], Step [40/58], Loss: 0.0021\n",
      "Epoch [252/500], Step [50/58], Loss: 0.0043\n",
      "Epoch [253/500], Step [10/58], Loss: 0.0033\n",
      "Epoch [253/500], Step [20/58], Loss: 0.0035\n",
      "Epoch [253/500], Step [30/58], Loss: 0.0038\n",
      "Epoch [253/500], Step [40/58], Loss: 0.0006\n",
      "Epoch [253/500], Step [50/58], Loss: 0.0013\n",
      "Epoch [254/500], Step [10/58], Loss: 0.0019\n",
      "Epoch [254/500], Step [20/58], Loss: 0.0026\n",
      "Epoch [254/500], Step [30/58], Loss: 0.0061\n",
      "Epoch [254/500], Step [40/58], Loss: 0.0050\n",
      "Epoch [254/500], Step [50/58], Loss: 0.0045\n",
      "Epoch [255/500], Step [10/58], Loss: 0.0043\n",
      "Epoch [255/500], Step [20/58], Loss: 0.0022\n",
      "Epoch [255/500], Step [30/58], Loss: 0.0012\n",
      "Epoch [255/500], Step [40/58], Loss: 0.0019\n",
      "Epoch [255/500], Step [50/58], Loss: 0.0039\n",
      "Epoch [256/500], Step [10/58], Loss: 0.0070\n",
      "Epoch [256/500], Step [20/58], Loss: 0.0035\n",
      "Epoch [256/500], Step [30/58], Loss: 0.0051\n",
      "Epoch [256/500], Step [40/58], Loss: 0.0090\n",
      "Epoch [256/500], Step [50/58], Loss: 0.0036\n",
      "Epoch [257/500], Step [10/58], Loss: 0.5166\n",
      "Epoch [257/500], Step [20/58], Loss: 0.2593\n",
      "Epoch [257/500], Step [30/58], Loss: 0.0971\n",
      "Epoch [257/500], Step [40/58], Loss: 0.1124\n",
      "Epoch [257/500], Step [50/58], Loss: 0.2114\n",
      "Epoch [258/500], Step [10/58], Loss: 0.3380\n",
      "Epoch [258/500], Step [20/58], Loss: 1.3006\n",
      "Epoch [258/500], Step [30/58], Loss: 1.0632\n",
      "Epoch [258/500], Step [40/58], Loss: 1.3391\n",
      "Epoch [258/500], Step [50/58], Loss: 1.3665\n",
      "Epoch [259/500], Step [10/58], Loss: 1.0570\n",
      "Epoch [259/500], Step [20/58], Loss: 0.8590\n",
      "Epoch [259/500], Step [30/58], Loss: 0.7141\n",
      "Epoch [259/500], Step [40/58], Loss: 0.5300\n",
      "Epoch [259/500], Step [50/58], Loss: 0.1900\n",
      "Epoch [260/500], Step [10/58], Loss: 0.1177\n",
      "Epoch [260/500], Step [20/58], Loss: 0.2397\n",
      "Epoch [260/500], Step [30/58], Loss: 0.2047\n",
      "Epoch [260/500], Step [40/58], Loss: 0.3786\n",
      "Epoch [260/500], Step [50/58], Loss: 0.1753\n",
      "Epoch [261/500], Step [10/58], Loss: 0.2644\n",
      "Epoch [261/500], Step [20/58], Loss: 0.0959\n",
      "Epoch [261/500], Step [30/58], Loss: 0.1607\n",
      "Epoch [261/500], Step [40/58], Loss: 0.2648\n",
      "Epoch [261/500], Step [50/58], Loss: 0.3201\n",
      "Epoch [262/500], Step [10/58], Loss: 0.1314\n",
      "Epoch [262/500], Step [20/58], Loss: 0.1425\n",
      "Epoch [262/500], Step [30/58], Loss: 0.1414\n",
      "Epoch [262/500], Step [40/58], Loss: 0.2030\n",
      "Epoch [262/500], Step [50/58], Loss: 0.9836\n",
      "Epoch [263/500], Step [10/58], Loss: 1.3269\n",
      "Epoch [263/500], Step [20/58], Loss: 0.9917\n",
      "Epoch [263/500], Step [30/58], Loss: 0.6657\n",
      "Epoch [263/500], Step [40/58], Loss: 0.5228\n",
      "Epoch [263/500], Step [50/58], Loss: 0.4183\n",
      "Epoch [264/500], Step [10/58], Loss: 0.2309\n",
      "Epoch [264/500], Step [20/58], Loss: 0.4666\n",
      "Epoch [264/500], Step [30/58], Loss: 0.1853\n",
      "Epoch [264/500], Step [40/58], Loss: 0.2416\n",
      "Epoch [264/500], Step [50/58], Loss: 0.2834\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [265/500], Step [10/58], Loss: 0.1667\n",
      "Epoch [265/500], Step [20/58], Loss: 0.1229\n",
      "Epoch [265/500], Step [30/58], Loss: 0.1623\n",
      "Epoch [265/500], Step [40/58], Loss: 0.0935\n",
      "Epoch [265/500], Step [50/58], Loss: 0.0701\n",
      "Epoch [266/500], Step [10/58], Loss: 0.1889\n",
      "Epoch [266/500], Step [20/58], Loss: 0.2917\n",
      "Epoch [266/500], Step [30/58], Loss: 0.0495\n",
      "Epoch [266/500], Step [40/58], Loss: 0.1637\n",
      "Epoch [266/500], Step [50/58], Loss: 0.0298\n",
      "Epoch [267/500], Step [10/58], Loss: 0.0715\n",
      "Epoch [267/500], Step [20/58], Loss: 0.1063\n",
      "Epoch [267/500], Step [30/58], Loss: 0.2011\n",
      "Epoch [267/500], Step [40/58], Loss: 0.0977\n",
      "Epoch [267/500], Step [50/58], Loss: 0.0639\n",
      "Epoch [268/500], Step [10/58], Loss: 0.0610\n",
      "Epoch [268/500], Step [20/58], Loss: 0.0116\n",
      "Epoch [268/500], Step [30/58], Loss: 0.1059\n",
      "Epoch [268/500], Step [40/58], Loss: 0.0367\n",
      "Epoch [268/500], Step [50/58], Loss: 0.0989\n",
      "Epoch [269/500], Step [10/58], Loss: 0.0539\n",
      "Epoch [269/500], Step [20/58], Loss: 0.0244\n",
      "Epoch [269/500], Step [30/58], Loss: 0.0550\n",
      "Epoch [269/500], Step [40/58], Loss: 0.0899\n",
      "Epoch [269/500], Step [50/58], Loss: 0.2320\n",
      "Epoch [270/500], Step [10/58], Loss: 0.0419\n",
      "Epoch [270/500], Step [20/58], Loss: 0.0104\n",
      "Epoch [270/500], Step [30/58], Loss: 0.1197\n",
      "Epoch [270/500], Step [40/58], Loss: 0.0271\n",
      "Epoch [270/500], Step [50/58], Loss: 0.0113\n",
      "Epoch [271/500], Step [10/58], Loss: 0.0777\n",
      "Epoch [271/500], Step [20/58], Loss: 0.1343\n",
      "Epoch [271/500], Step [30/58], Loss: 0.0107\n",
      "Epoch [271/500], Step [40/58], Loss: 0.1578\n",
      "Epoch [271/500], Step [50/58], Loss: 0.0318\n",
      "Epoch [272/500], Step [10/58], Loss: 0.1324\n",
      "Epoch [272/500], Step [20/58], Loss: 0.0400\n",
      "Epoch [272/500], Step [30/58], Loss: 0.0742\n",
      "Epoch [272/500], Step [40/58], Loss: 0.1266\n",
      "Epoch [272/500], Step [50/58], Loss: 0.2137\n",
      "Epoch [273/500], Step [10/58], Loss: 0.0293\n",
      "Epoch [273/500], Step [20/58], Loss: 0.0703\n",
      "Epoch [273/500], Step [30/58], Loss: 0.0178\n",
      "Epoch [273/500], Step [40/58], Loss: 0.0476\n",
      "Epoch [273/500], Step [50/58], Loss: 0.0630\n",
      "Epoch [274/500], Step [10/58], Loss: 0.3019\n",
      "Epoch [274/500], Step [20/58], Loss: 0.6677\n",
      "Epoch [274/500], Step [30/58], Loss: 0.1300\n",
      "Epoch [274/500], Step [40/58], Loss: 0.2284\n",
      "Epoch [274/500], Step [50/58], Loss: 0.3089\n",
      "Epoch [275/500], Step [10/58], Loss: 0.0777\n",
      "Epoch [275/500], Step [20/58], Loss: 0.1363\n",
      "Epoch [275/500], Step [30/58], Loss: 0.1448\n",
      "Epoch [275/500], Step [40/58], Loss: 0.0736\n",
      "Epoch [275/500], Step [50/58], Loss: 0.1782\n",
      "Epoch [276/500], Step [10/58], Loss: 0.1770\n",
      "Epoch [276/500], Step [20/58], Loss: 0.3121\n",
      "Epoch [276/500], Step [30/58], Loss: 9.9188\n",
      "Epoch [276/500], Step [40/58], Loss: 5.1450\n",
      "Epoch [276/500], Step [50/58], Loss: 2.9266\n",
      "Epoch [277/500], Step [10/58], Loss: 2.3480\n",
      "Epoch [277/500], Step [20/58], Loss: 2.3742\n",
      "Epoch [277/500], Step [30/58], Loss: 2.4763\n",
      "Epoch [277/500], Step [40/58], Loss: 2.3229\n",
      "Epoch [277/500], Step [50/58], Loss: 2.3440\n",
      "Epoch [278/500], Step [10/58], Loss: 2.3447\n",
      "Epoch [278/500], Step [20/58], Loss: 2.3714\n",
      "Epoch [278/500], Step [30/58], Loss: 2.3400\n",
      "Epoch [278/500], Step [40/58], Loss: 2.3020\n",
      "Epoch [278/500], Step [50/58], Loss: 2.3433\n",
      "Epoch [279/500], Step [10/58], Loss: 2.3090\n",
      "Epoch [279/500], Step [20/58], Loss: 2.2936\n",
      "Epoch [279/500], Step [30/58], Loss: 2.2976\n",
      "Epoch [279/500], Step [40/58], Loss: 2.2308\n",
      "Epoch [279/500], Step [50/58], Loss: 2.3275\n",
      "Epoch [280/500], Step [10/58], Loss: 2.2803\n",
      "Epoch [280/500], Step [20/58], Loss: 2.2706\n",
      "Epoch [280/500], Step [30/58], Loss: 2.2709\n",
      "Epoch [280/500], Step [40/58], Loss: 2.2764\n",
      "Epoch [280/500], Step [50/58], Loss: 2.1907\n",
      "Epoch [281/500], Step [10/58], Loss: 2.2188\n",
      "Epoch [281/500], Step [20/58], Loss: 2.3023\n",
      "Epoch [281/500], Step [30/58], Loss: 2.2281\n",
      "Epoch [281/500], Step [40/58], Loss: 2.0672\n",
      "Epoch [281/500], Step [50/58], Loss: 2.3283\n",
      "Epoch [282/500], Step [10/58], Loss: 2.2654\n",
      "Epoch [282/500], Step [20/58], Loss: 2.2055\n",
      "Epoch [282/500], Step [30/58], Loss: 2.1401\n",
      "Epoch [282/500], Step [40/58], Loss: 2.3348\n",
      "Epoch [282/500], Step [50/58], Loss: 2.0992\n",
      "Epoch [283/500], Step [10/58], Loss: 2.0941\n",
      "Epoch [283/500], Step [20/58], Loss: 2.2165\n",
      "Epoch [283/500], Step [30/58], Loss: 2.1776\n",
      "Epoch [283/500], Step [40/58], Loss: 2.3737\n",
      "Epoch [283/500], Step [50/58], Loss: 2.1506\n",
      "Epoch [284/500], Step [10/58], Loss: 2.2403\n",
      "Epoch [284/500], Step [20/58], Loss: 1.9686\n",
      "Epoch [284/500], Step [30/58], Loss: 2.0676\n",
      "Epoch [284/500], Step [40/58], Loss: 2.2778\n",
      "Epoch [284/500], Step [50/58], Loss: 2.2171\n",
      "Epoch [285/500], Step [10/58], Loss: 2.1569\n",
      "Epoch [285/500], Step [20/58], Loss: 2.2287\n",
      "Epoch [285/500], Step [30/58], Loss: 2.0692\n",
      "Epoch [285/500], Step [40/58], Loss: 2.2333\n",
      "Epoch [285/500], Step [50/58], Loss: 2.2965\n",
      "Epoch [286/500], Step [10/58], Loss: 2.2439\n",
      "Epoch [286/500], Step [20/58], Loss: 2.1108\n",
      "Epoch [286/500], Step [30/58], Loss: 2.2072\n",
      "Epoch [286/500], Step [40/58], Loss: 2.2352\n",
      "Epoch [286/500], Step [50/58], Loss: 2.0047\n",
      "Epoch [287/500], Step [10/58], Loss: 2.2157\n",
      "Epoch [287/500], Step [20/58], Loss: 2.1694\n",
      "Epoch [287/500], Step [30/58], Loss: 2.2898\n",
      "Epoch [287/500], Step [40/58], Loss: 2.2798\n",
      "Epoch [287/500], Step [50/58], Loss: 2.0972\n",
      "Epoch [288/500], Step [10/58], Loss: 2.1520\n",
      "Epoch [288/500], Step [20/58], Loss: 2.1794\n",
      "Epoch [288/500], Step [30/58], Loss: 2.2779\n",
      "Epoch [288/500], Step [40/58], Loss: 1.9309\n",
      "Epoch [288/500], Step [50/58], Loss: 2.0559\n",
      "Epoch [289/500], Step [10/58], Loss: 2.0081\n",
      "Epoch [289/500], Step [20/58], Loss: 2.0526\n",
      "Epoch [289/500], Step [30/58], Loss: 2.0109\n",
      "Epoch [289/500], Step [40/58], Loss: 1.9737\n",
      "Epoch [289/500], Step [50/58], Loss: 2.0988\n",
      "Epoch [290/500], Step [10/58], Loss: 2.2977\n",
      "Epoch [290/500], Step [20/58], Loss: 2.4550\n",
      "Epoch [290/500], Step [30/58], Loss: 2.1495\n",
      "Epoch [290/500], Step [40/58], Loss: 2.3225\n",
      "Epoch [290/500], Step [50/58], Loss: 1.9857\n",
      "Epoch [291/500], Step [10/58], Loss: 2.2507\n",
      "Epoch [291/500], Step [20/58], Loss: 2.2042\n",
      "Epoch [291/500], Step [30/58], Loss: 2.1090\n",
      "Epoch [291/500], Step [40/58], Loss: 2.2666\n",
      "Epoch [291/500], Step [50/58], Loss: 2.2923\n",
      "Epoch [292/500], Step [10/58], Loss: 2.0015\n",
      "Epoch [292/500], Step [20/58], Loss: 2.2172\n",
      "Epoch [292/500], Step [30/58], Loss: 2.2405\n",
      "Epoch [292/500], Step [40/58], Loss: 2.2259\n",
      "Epoch [292/500], Step [50/58], Loss: 2.2763\n",
      "Epoch [293/500], Step [10/58], Loss: 2.2036\n",
      "Epoch [293/500], Step [20/58], Loss: 2.0819\n",
      "Epoch [293/500], Step [30/58], Loss: 2.1154\n",
      "Epoch [293/500], Step [40/58], Loss: 2.1372\n",
      "Epoch [293/500], Step [50/58], Loss: 2.2849\n",
      "Epoch [294/500], Step [10/58], Loss: 1.8594\n",
      "Epoch [294/500], Step [20/58], Loss: 2.0939\n",
      "Epoch [294/500], Step [30/58], Loss: 2.2837\n",
      "Epoch [294/500], Step [40/58], Loss: 2.1439\n",
      "Epoch [294/500], Step [50/58], Loss: 1.9193\n",
      "Epoch [295/500], Step [10/58], Loss: 2.0455\n",
      "Epoch [295/500], Step [20/58], Loss: 1.9465\n",
      "Epoch [295/500], Step [30/58], Loss: 2.0309\n",
      "Epoch [295/500], Step [40/58], Loss: 2.1195\n",
      "Epoch [295/500], Step [50/58], Loss: 2.0049\n",
      "Epoch [296/500], Step [10/58], Loss: 2.2234\n",
      "Epoch [296/500], Step [20/58], Loss: 1.8110\n",
      "Epoch [296/500], Step [30/58], Loss: 2.0052\n",
      "Epoch [296/500], Step [40/58], Loss: 2.0684\n",
      "Epoch [296/500], Step [50/58], Loss: 1.8469\n",
      "Epoch [297/500], Step [10/58], Loss: 2.1922\n",
      "Epoch [297/500], Step [20/58], Loss: 1.9195\n",
      "Epoch [297/500], Step [30/58], Loss: 2.0151\n",
      "Epoch [297/500], Step [40/58], Loss: 1.7654\n",
      "Epoch [297/500], Step [50/58], Loss: 1.9318\n",
      "Epoch [298/500], Step [10/58], Loss: 1.8182\n",
      "Epoch [298/500], Step [20/58], Loss: 1.7825\n",
      "Epoch [298/500], Step [30/58], Loss: 2.1149\n",
      "Epoch [298/500], Step [40/58], Loss: 1.9518\n",
      "Epoch [298/500], Step [50/58], Loss: 2.2129\n",
      "Epoch [299/500], Step [10/58], Loss: 1.8792\n",
      "Epoch [299/500], Step [20/58], Loss: 1.7983\n",
      "Epoch [299/500], Step [30/58], Loss: 1.7931\n",
      "Epoch [299/500], Step [40/58], Loss: 1.9906\n",
      "Epoch [299/500], Step [50/58], Loss: 1.9239\n",
      "Epoch [300/500], Step [10/58], Loss: 2.0233\n",
      "Epoch [300/500], Step [20/58], Loss: 1.8372\n",
      "Epoch [300/500], Step [30/58], Loss: 2.0907\n",
      "Epoch [300/500], Step [40/58], Loss: 2.0823\n",
      "Epoch [300/500], Step [50/58], Loss: 1.9690\n",
      "Epoch [301/500], Step [10/58], Loss: 1.8789\n",
      "Epoch [301/500], Step [20/58], Loss: 1.9248\n",
      "Epoch [301/500], Step [30/58], Loss: 1.8725\n",
      "Epoch [301/500], Step [40/58], Loss: 2.0886\n",
      "Epoch [301/500], Step [50/58], Loss: 1.8932\n",
      "Epoch [302/500], Step [10/58], Loss: 2.1362\n",
      "Epoch [302/500], Step [20/58], Loss: 1.6475\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [302/500], Step [30/58], Loss: 1.8486\n",
      "Epoch [302/500], Step [40/58], Loss: 1.8176\n",
      "Epoch [302/500], Step [50/58], Loss: 1.9638\n",
      "Epoch [303/500], Step [10/58], Loss: 1.6291\n",
      "Epoch [303/500], Step [20/58], Loss: 1.8739\n",
      "Epoch [303/500], Step [30/58], Loss: 1.9328\n",
      "Epoch [303/500], Step [40/58], Loss: 2.0417\n",
      "Epoch [303/500], Step [50/58], Loss: 1.8961\n",
      "Epoch [304/500], Step [10/58], Loss: 1.8000\n",
      "Epoch [304/500], Step [20/58], Loss: 1.9342\n",
      "Epoch [304/500], Step [30/58], Loss: 1.7126\n",
      "Epoch [304/500], Step [40/58], Loss: 1.7568\n",
      "Epoch [304/500], Step [50/58], Loss: 1.8191\n",
      "Epoch [305/500], Step [10/58], Loss: 1.8163\n",
      "Epoch [305/500], Step [20/58], Loss: 1.8930\n",
      "Epoch [305/500], Step [30/58], Loss: 1.8504\n",
      "Epoch [305/500], Step [40/58], Loss: 1.9352\n",
      "Epoch [305/500], Step [50/58], Loss: 1.6119\n",
      "Epoch [306/500], Step [10/58], Loss: 1.8589\n",
      "Epoch [306/500], Step [20/58], Loss: 1.8439\n",
      "Epoch [306/500], Step [30/58], Loss: 1.8928\n",
      "Epoch [306/500], Step [40/58], Loss: 2.0711\n",
      "Epoch [306/500], Step [50/58], Loss: 1.7604\n",
      "Epoch [307/500], Step [10/58], Loss: 1.9287\n",
      "Epoch [307/500], Step [20/58], Loss: 1.8143\n",
      "Epoch [307/500], Step [30/58], Loss: 1.9880\n",
      "Epoch [307/500], Step [40/58], Loss: 1.7273\n",
      "Epoch [307/500], Step [50/58], Loss: 1.7290\n",
      "Epoch [308/500], Step [10/58], Loss: 1.7101\n",
      "Epoch [308/500], Step [20/58], Loss: 1.6416\n",
      "Epoch [308/500], Step [30/58], Loss: 1.7664\n",
      "Epoch [308/500], Step [40/58], Loss: 2.2026\n",
      "Epoch [308/500], Step [50/58], Loss: 1.5300\n",
      "Epoch [309/500], Step [10/58], Loss: 2.0383\n",
      "Epoch [309/500], Step [20/58], Loss: 1.8256\n",
      "Epoch [309/500], Step [30/58], Loss: 1.9642\n",
      "Epoch [309/500], Step [40/58], Loss: 2.0746\n",
      "Epoch [309/500], Step [50/58], Loss: 1.6229\n",
      "Epoch [310/500], Step [10/58], Loss: 1.9651\n",
      "Epoch [310/500], Step [20/58], Loss: 1.7368\n",
      "Epoch [310/500], Step [30/58], Loss: 1.7328\n",
      "Epoch [310/500], Step [40/58], Loss: 1.9324\n",
      "Epoch [310/500], Step [50/58], Loss: 1.8882\n",
      "Epoch [311/500], Step [10/58], Loss: 1.7773\n",
      "Epoch [311/500], Step [20/58], Loss: 1.8072\n",
      "Epoch [311/500], Step [30/58], Loss: 1.9925\n",
      "Epoch [311/500], Step [40/58], Loss: 2.0199\n",
      "Epoch [311/500], Step [50/58], Loss: 1.7693\n",
      "Epoch [312/500], Step [10/58], Loss: 1.8634\n",
      "Epoch [312/500], Step [20/58], Loss: 1.8116\n",
      "Epoch [312/500], Step [30/58], Loss: 1.7954\n",
      "Epoch [312/500], Step [40/58], Loss: 2.0207\n",
      "Epoch [312/500], Step [50/58], Loss: 1.8173\n",
      "Epoch [313/500], Step [10/58], Loss: 1.5256\n",
      "Epoch [313/500], Step [20/58], Loss: 1.8032\n",
      "Epoch [313/500], Step [30/58], Loss: 1.5623\n",
      "Epoch [313/500], Step [40/58], Loss: 1.3112\n",
      "Epoch [313/500], Step [50/58], Loss: 1.7110\n",
      "Epoch [314/500], Step [10/58], Loss: 1.7090\n",
      "Epoch [314/500], Step [20/58], Loss: 1.7323\n",
      "Epoch [314/500], Step [30/58], Loss: 1.7131\n",
      "Epoch [314/500], Step [40/58], Loss: 1.4973\n",
      "Epoch [314/500], Step [50/58], Loss: 2.0090\n",
      "Epoch [315/500], Step [10/58], Loss: 1.6479\n",
      "Epoch [315/500], Step [20/58], Loss: 1.7056\n",
      "Epoch [315/500], Step [30/58], Loss: 1.6946\n",
      "Epoch [315/500], Step [40/58], Loss: 1.6718\n",
      "Epoch [315/500], Step [50/58], Loss: 1.8799\n",
      "Epoch [316/500], Step [10/58], Loss: 1.7871\n",
      "Epoch [316/500], Step [20/58], Loss: 1.8011\n",
      "Epoch [316/500], Step [30/58], Loss: 1.6417\n",
      "Epoch [316/500], Step [40/58], Loss: 1.7266\n",
      "Epoch [316/500], Step [50/58], Loss: 1.5337\n",
      "Epoch [317/500], Step [10/58], Loss: 1.6895\n",
      "Epoch [317/500], Step [20/58], Loss: 1.7090\n",
      "Epoch [317/500], Step [30/58], Loss: 1.8200\n",
      "Epoch [317/500], Step [40/58], Loss: 1.9145\n",
      "Epoch [317/500], Step [50/58], Loss: 1.7384\n",
      "Epoch [318/500], Step [10/58], Loss: 1.7862\n",
      "Epoch [318/500], Step [20/58], Loss: 1.6714\n",
      "Epoch [318/500], Step [30/58], Loss: 1.7444\n",
      "Epoch [318/500], Step [40/58], Loss: 1.8661\n",
      "Epoch [318/500], Step [50/58], Loss: 1.8646\n",
      "Epoch [319/500], Step [10/58], Loss: 1.7949\n",
      "Epoch [319/500], Step [20/58], Loss: 1.6079\n",
      "Epoch [319/500], Step [30/58], Loss: 1.5502\n",
      "Epoch [319/500], Step [40/58], Loss: 1.3201\n",
      "Epoch [319/500], Step [50/58], Loss: 1.5728\n",
      "Epoch [320/500], Step [10/58], Loss: 1.6669\n",
      "Epoch [320/500], Step [20/58], Loss: 1.7972\n",
      "Epoch [320/500], Step [30/58], Loss: 1.8101\n",
      "Epoch [320/500], Step [40/58], Loss: 1.3843\n",
      "Epoch [320/500], Step [50/58], Loss: 1.4200\n",
      "Epoch [321/500], Step [10/58], Loss: 1.3839\n",
      "Epoch [321/500], Step [20/58], Loss: 1.3812\n",
      "Epoch [321/500], Step [30/58], Loss: 1.7746\n",
      "Epoch [321/500], Step [40/58], Loss: 1.7348\n",
      "Epoch [321/500], Step [50/58], Loss: 1.7035\n",
      "Epoch [322/500], Step [10/58], Loss: 1.5496\n",
      "Epoch [322/500], Step [20/58], Loss: 1.7830\n",
      "Epoch [322/500], Step [30/58], Loss: 1.9495\n",
      "Epoch [322/500], Step [40/58], Loss: 1.8477\n",
      "Epoch [322/500], Step [50/58], Loss: 1.4396\n",
      "Epoch [323/500], Step [10/58], Loss: 2.8741\n",
      "Epoch [323/500], Step [20/58], Loss: 1.9931\n",
      "Epoch [323/500], Step [30/58], Loss: 1.7292\n",
      "Epoch [323/500], Step [40/58], Loss: 1.7840\n",
      "Epoch [323/500], Step [50/58], Loss: 1.5926\n",
      "Epoch [324/500], Step [10/58], Loss: 1.8732\n",
      "Epoch [324/500], Step [20/58], Loss: 1.5258\n",
      "Epoch [324/500], Step [30/58], Loss: 1.8654\n",
      "Epoch [324/500], Step [40/58], Loss: 1.5617\n",
      "Epoch [324/500], Step [50/58], Loss: 1.6783\n",
      "Epoch [325/500], Step [10/58], Loss: 1.6352\n",
      "Epoch [325/500], Step [20/58], Loss: 1.7744\n",
      "Epoch [325/500], Step [30/58], Loss: 1.5340\n",
      "Epoch [325/500], Step [40/58], Loss: 1.6205\n",
      "Epoch [325/500], Step [50/58], Loss: 1.8429\n",
      "Epoch [326/500], Step [10/58], Loss: 1.6030\n",
      "Epoch [326/500], Step [20/58], Loss: 1.5609\n",
      "Epoch [326/500], Step [30/58], Loss: 1.6015\n",
      "Epoch [326/500], Step [40/58], Loss: 1.6229\n",
      "Epoch [326/500], Step [50/58], Loss: 1.4402\n",
      "Epoch [327/500], Step [10/58], Loss: 1.7370\n",
      "Epoch [327/500], Step [20/58], Loss: 1.6277\n",
      "Epoch [327/500], Step [30/58], Loss: 1.5725\n",
      "Epoch [327/500], Step [40/58], Loss: 1.7077\n",
      "Epoch [327/500], Step [50/58], Loss: 1.6961\n",
      "Epoch [328/500], Step [10/58], Loss: 1.7125\n",
      "Epoch [328/500], Step [20/58], Loss: 1.7278\n",
      "Epoch [328/500], Step [30/58], Loss: 1.8223\n",
      "Epoch [328/500], Step [40/58], Loss: 1.6239\n",
      "Epoch [328/500], Step [50/58], Loss: 1.9278\n",
      "Epoch [329/500], Step [10/58], Loss: 1.6781\n",
      "Epoch [329/500], Step [20/58], Loss: 1.9885\n",
      "Epoch [329/500], Step [30/58], Loss: 1.8284\n",
      "Epoch [329/500], Step [40/58], Loss: 1.8874\n",
      "Epoch [329/500], Step [50/58], Loss: 1.6760\n",
      "Epoch [330/500], Step [10/58], Loss: 1.7881\n",
      "Epoch [330/500], Step [20/58], Loss: 1.5064\n",
      "Epoch [330/500], Step [30/58], Loss: 1.5512\n",
      "Epoch [330/500], Step [40/58], Loss: 1.4867\n",
      "Epoch [330/500], Step [50/58], Loss: 1.8679\n",
      "Epoch [331/500], Step [10/58], Loss: 1.8520\n",
      "Epoch [331/500], Step [20/58], Loss: 1.7648\n",
      "Epoch [331/500], Step [30/58], Loss: 1.5799\n",
      "Epoch [331/500], Step [40/58], Loss: 1.5961\n",
      "Epoch [331/500], Step [50/58], Loss: 1.5204\n",
      "Epoch [332/500], Step [10/58], Loss: 1.4925\n",
      "Epoch [332/500], Step [20/58], Loss: 1.5924\n",
      "Epoch [332/500], Step [30/58], Loss: 1.6478\n",
      "Epoch [332/500], Step [40/58], Loss: 1.4175\n",
      "Epoch [332/500], Step [50/58], Loss: 1.6330\n",
      "Epoch [333/500], Step [10/58], Loss: 1.8067\n",
      "Epoch [333/500], Step [20/58], Loss: 1.7983\n",
      "Epoch [333/500], Step [30/58], Loss: 1.6019\n",
      "Epoch [333/500], Step [40/58], Loss: 1.6805\n",
      "Epoch [333/500], Step [50/58], Loss: 1.4132\n",
      "Epoch [334/500], Step [10/58], Loss: 1.6829\n",
      "Epoch [334/500], Step [20/58], Loss: 1.5785\n",
      "Epoch [334/500], Step [30/58], Loss: 1.5580\n",
      "Epoch [334/500], Step [40/58], Loss: 1.5935\n",
      "Epoch [334/500], Step [50/58], Loss: 1.6162\n",
      "Epoch [335/500], Step [10/58], Loss: 1.6820\n",
      "Epoch [335/500], Step [20/58], Loss: 1.3636\n",
      "Epoch [335/500], Step [30/58], Loss: 1.7932\n",
      "Epoch [335/500], Step [40/58], Loss: 1.6233\n",
      "Epoch [335/500], Step [50/58], Loss: 1.4619\n",
      "Epoch [336/500], Step [10/58], Loss: 1.6786\n",
      "Epoch [336/500], Step [20/58], Loss: 1.8048\n",
      "Epoch [336/500], Step [30/58], Loss: 1.5653\n",
      "Epoch [336/500], Step [40/58], Loss: 1.7981\n",
      "Epoch [336/500], Step [50/58], Loss: 1.5338\n",
      "Epoch [337/500], Step [10/58], Loss: 1.7702\n",
      "Epoch [337/500], Step [20/58], Loss: 1.5307\n",
      "Epoch [337/500], Step [30/58], Loss: 1.5291\n",
      "Epoch [337/500], Step [40/58], Loss: 1.5331\n",
      "Epoch [337/500], Step [50/58], Loss: 1.7557\n",
      "Epoch [338/500], Step [10/58], Loss: 1.3622\n",
      "Epoch [338/500], Step [20/58], Loss: 1.5452\n",
      "Epoch [338/500], Step [30/58], Loss: 1.6593\n",
      "Epoch [338/500], Step [40/58], Loss: 1.6378\n",
      "Epoch [338/500], Step [50/58], Loss: 1.6413\n",
      "Epoch [339/500], Step [10/58], Loss: 1.2388\n",
      "Epoch [339/500], Step [20/58], Loss: 1.8113\n",
      "Epoch [339/500], Step [30/58], Loss: 1.5947\n",
      "Epoch [339/500], Step [40/58], Loss: 1.5801\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [339/500], Step [50/58], Loss: 1.7211\n",
      "Epoch [340/500], Step [10/58], Loss: 1.3646\n",
      "Epoch [340/500], Step [20/58], Loss: 1.7747\n",
      "Epoch [340/500], Step [30/58], Loss: 1.6948\n",
      "Epoch [340/500], Step [40/58], Loss: 1.6087\n",
      "Epoch [340/500], Step [50/58], Loss: 1.7300\n",
      "Epoch [341/500], Step [10/58], Loss: 1.5883\n",
      "Epoch [341/500], Step [20/58], Loss: 1.5059\n",
      "Epoch [341/500], Step [30/58], Loss: 1.5725\n",
      "Epoch [341/500], Step [40/58], Loss: 1.4987\n",
      "Epoch [341/500], Step [50/58], Loss: 1.3019\n",
      "Epoch [342/500], Step [10/58], Loss: 1.3150\n",
      "Epoch [342/500], Step [20/58], Loss: 1.8714\n",
      "Epoch [342/500], Step [30/58], Loss: 1.5440\n",
      "Epoch [342/500], Step [40/58], Loss: 1.4896\n",
      "Epoch [342/500], Step [50/58], Loss: 1.7826\n",
      "Epoch [343/500], Step [10/58], Loss: 1.3792\n",
      "Epoch [343/500], Step [20/58], Loss: 1.5434\n",
      "Epoch [343/500], Step [30/58], Loss: 1.6569\n",
      "Epoch [343/500], Step [40/58], Loss: 1.4723\n",
      "Epoch [343/500], Step [50/58], Loss: 1.7035\n",
      "Epoch [344/500], Step [10/58], Loss: 1.7046\n",
      "Epoch [344/500], Step [20/58], Loss: 1.7684\n",
      "Epoch [344/500], Step [30/58], Loss: 1.5962\n",
      "Epoch [344/500], Step [40/58], Loss: 1.6521\n",
      "Epoch [344/500], Step [50/58], Loss: 1.7211\n",
      "Epoch [345/500], Step [10/58], Loss: 1.5752\n",
      "Epoch [345/500], Step [20/58], Loss: 1.5744\n",
      "Epoch [345/500], Step [30/58], Loss: 1.8117\n",
      "Epoch [345/500], Step [40/58], Loss: 1.7020\n",
      "Epoch [345/500], Step [50/58], Loss: 1.4302\n",
      "Epoch [346/500], Step [10/58], Loss: 1.7171\n",
      "Epoch [346/500], Step [20/58], Loss: 1.5707\n",
      "Epoch [346/500], Step [30/58], Loss: 1.6632\n",
      "Epoch [346/500], Step [40/58], Loss: 1.6617\n",
      "Epoch [346/500], Step [50/58], Loss: 1.6339\n",
      "Epoch [347/500], Step [10/58], Loss: 1.7738\n",
      "Epoch [347/500], Step [20/58], Loss: 1.4880\n",
      "Epoch [347/500], Step [30/58], Loss: 1.3379\n",
      "Epoch [347/500], Step [40/58], Loss: 1.8502\n",
      "Epoch [347/500], Step [50/58], Loss: 1.4361\n",
      "Epoch [348/500], Step [10/58], Loss: 1.4985\n",
      "Epoch [348/500], Step [20/58], Loss: 1.8434\n",
      "Epoch [348/500], Step [30/58], Loss: 1.5031\n",
      "Epoch [348/500], Step [40/58], Loss: 1.4641\n",
      "Epoch [348/500], Step [50/58], Loss: 1.3971\n",
      "Epoch [349/500], Step [10/58], Loss: 1.5171\n",
      "Epoch [349/500], Step [20/58], Loss: 1.7293\n",
      "Epoch [349/500], Step [30/58], Loss: 1.6985\n",
      "Epoch [349/500], Step [40/58], Loss: 1.3146\n",
      "Epoch [349/500], Step [50/58], Loss: 1.4150\n",
      "Epoch [350/500], Step [10/58], Loss: 1.4592\n",
      "Epoch [350/500], Step [20/58], Loss: 1.4426\n",
      "Epoch [350/500], Step [30/58], Loss: 1.6563\n",
      "Epoch [350/500], Step [40/58], Loss: 1.7337\n",
      "Epoch [350/500], Step [50/58], Loss: 1.5059\n",
      "Epoch [351/500], Step [10/58], Loss: 1.7142\n",
      "Epoch [351/500], Step [20/58], Loss: 1.4016\n",
      "Epoch [351/500], Step [30/58], Loss: 1.4857\n",
      "Epoch [351/500], Step [40/58], Loss: 1.5570\n",
      "Epoch [351/500], Step [50/58], Loss: 1.6238\n",
      "Epoch [352/500], Step [10/58], Loss: 1.4321\n",
      "Epoch [352/500], Step [20/58], Loss: 1.6683\n",
      "Epoch [352/500], Step [30/58], Loss: 1.4526\n",
      "Epoch [352/500], Step [40/58], Loss: 1.5968\n",
      "Epoch [352/500], Step [50/58], Loss: 1.6036\n",
      "Epoch [353/500], Step [10/58], Loss: 1.5507\n",
      "Epoch [353/500], Step [20/58], Loss: 1.3947\n",
      "Epoch [353/500], Step [30/58], Loss: 1.6183\n",
      "Epoch [353/500], Step [40/58], Loss: 1.5623\n",
      "Epoch [353/500], Step [50/58], Loss: 1.3455\n",
      "Epoch [354/500], Step [10/58], Loss: 1.7203\n",
      "Epoch [354/500], Step [20/58], Loss: 1.2906\n",
      "Epoch [354/500], Step [30/58], Loss: 1.4014\n",
      "Epoch [354/500], Step [40/58], Loss: 1.2667\n",
      "Epoch [354/500], Step [50/58], Loss: 1.4037\n",
      "Epoch [355/500], Step [10/58], Loss: 1.5788\n",
      "Epoch [355/500], Step [20/58], Loss: 1.6160\n",
      "Epoch [355/500], Step [30/58], Loss: 1.5131\n",
      "Epoch [355/500], Step [40/58], Loss: 1.6379\n",
      "Epoch [355/500], Step [50/58], Loss: 1.1780\n",
      "Epoch [356/500], Step [10/58], Loss: 1.6034\n",
      "Epoch [356/500], Step [20/58], Loss: 1.7122\n",
      "Epoch [356/500], Step [30/58], Loss: 1.2674\n",
      "Epoch [356/500], Step [40/58], Loss: 1.5188\n",
      "Epoch [356/500], Step [50/58], Loss: 1.4022\n",
      "Epoch [357/500], Step [10/58], Loss: 1.5341\n",
      "Epoch [357/500], Step [20/58], Loss: 1.3568\n",
      "Epoch [357/500], Step [30/58], Loss: 1.4495\n",
      "Epoch [357/500], Step [40/58], Loss: 1.3684\n",
      "Epoch [357/500], Step [50/58], Loss: 1.6992\n",
      "Epoch [358/500], Step [10/58], Loss: 1.5423\n",
      "Epoch [358/500], Step [20/58], Loss: 1.5676\n",
      "Epoch [358/500], Step [30/58], Loss: 1.1007\n",
      "Epoch [358/500], Step [40/58], Loss: 1.4456\n",
      "Epoch [358/500], Step [50/58], Loss: 1.3637\n",
      "Epoch [359/500], Step [10/58], Loss: 1.5165\n",
      "Epoch [359/500], Step [20/58], Loss: 1.7012\n",
      "Epoch [359/500], Step [30/58], Loss: 1.2600\n",
      "Epoch [359/500], Step [40/58], Loss: 1.1643\n",
      "Epoch [359/500], Step [50/58], Loss: 1.2972\n",
      "Epoch [360/500], Step [10/58], Loss: 1.4017\n",
      "Epoch [360/500], Step [20/58], Loss: 1.3454\n",
      "Epoch [360/500], Step [30/58], Loss: 1.4311\n",
      "Epoch [360/500], Step [40/58], Loss: 1.7229\n",
      "Epoch [360/500], Step [50/58], Loss: 1.7008\n",
      "Epoch [361/500], Step [10/58], Loss: 1.4643\n",
      "Epoch [361/500], Step [20/58], Loss: 1.7341\n",
      "Epoch [361/500], Step [30/58], Loss: 1.5379\n",
      "Epoch [361/500], Step [40/58], Loss: 1.3541\n",
      "Epoch [361/500], Step [50/58], Loss: 1.4254\n",
      "Epoch [362/500], Step [10/58], Loss: 1.5205\n",
      "Epoch [362/500], Step [20/58], Loss: 1.4161\n",
      "Epoch [362/500], Step [30/58], Loss: 1.5977\n",
      "Epoch [362/500], Step [40/58], Loss: 1.2847\n",
      "Epoch [362/500], Step [50/58], Loss: 1.5384\n",
      "Epoch [363/500], Step [10/58], Loss: 1.5504\n",
      "Epoch [363/500], Step [20/58], Loss: 1.3990\n",
      "Epoch [363/500], Step [30/58], Loss: 1.2860\n",
      "Epoch [363/500], Step [40/58], Loss: 1.3597\n",
      "Epoch [363/500], Step [50/58], Loss: 1.1550\n",
      "Epoch [364/500], Step [10/58], Loss: 1.5141\n",
      "Epoch [364/500], Step [20/58], Loss: 1.4891\n",
      "Epoch [364/500], Step [30/58], Loss: 1.3152\n",
      "Epoch [364/500], Step [40/58], Loss: 1.2190\n",
      "Epoch [364/500], Step [50/58], Loss: 1.4480\n",
      "Epoch [365/500], Step [10/58], Loss: 1.4773\n",
      "Epoch [365/500], Step [20/58], Loss: 1.7286\n",
      "Epoch [365/500], Step [30/58], Loss: 1.5441\n",
      "Epoch [365/500], Step [40/58], Loss: 1.5143\n",
      "Epoch [365/500], Step [50/58], Loss: 1.5057\n",
      "Epoch [366/500], Step [10/58], Loss: 1.5149\n",
      "Epoch [366/500], Step [20/58], Loss: 1.0981\n",
      "Epoch [366/500], Step [30/58], Loss: 1.4809\n",
      "Epoch [366/500], Step [40/58], Loss: 1.1335\n",
      "Epoch [366/500], Step [50/58], Loss: 1.5107\n",
      "Epoch [367/500], Step [10/58], Loss: 1.4827\n",
      "Epoch [367/500], Step [20/58], Loss: 1.4398\n",
      "Epoch [367/500], Step [30/58], Loss: 1.1930\n",
      "Epoch [367/500], Step [40/58], Loss: 1.4034\n",
      "Epoch [367/500], Step [50/58], Loss: 1.4500\n",
      "Epoch [368/500], Step [10/58], Loss: 1.3570\n",
      "Epoch [368/500], Step [20/58], Loss: 1.5833\n",
      "Epoch [368/500], Step [30/58], Loss: 1.3142\n",
      "Epoch [368/500], Step [40/58], Loss: 1.4455\n",
      "Epoch [368/500], Step [50/58], Loss: 1.0792\n",
      "Epoch [369/500], Step [10/58], Loss: 1.4812\n",
      "Epoch [369/500], Step [20/58], Loss: 1.4390\n",
      "Epoch [369/500], Step [30/58], Loss: 1.4320\n",
      "Epoch [369/500], Step [40/58], Loss: 1.6762\n",
      "Epoch [369/500], Step [50/58], Loss: 1.4177\n",
      "Epoch [370/500], Step [10/58], Loss: 1.3976\n",
      "Epoch [370/500], Step [20/58], Loss: 1.6087\n",
      "Epoch [370/500], Step [30/58], Loss: 1.3918\n",
      "Epoch [370/500], Step [40/58], Loss: 1.4203\n",
      "Epoch [370/500], Step [50/58], Loss: 1.5779\n",
      "Epoch [371/500], Step [10/58], Loss: 1.4656\n",
      "Epoch [371/500], Step [20/58], Loss: 1.3042\n",
      "Epoch [371/500], Step [30/58], Loss: 1.2333\n",
      "Epoch [371/500], Step [40/58], Loss: 1.7406\n",
      "Epoch [371/500], Step [50/58], Loss: 1.4599\n",
      "Epoch [372/500], Step [10/58], Loss: 1.2234\n",
      "Epoch [372/500], Step [20/58], Loss: 1.5319\n",
      "Epoch [372/500], Step [30/58], Loss: 1.5084\n",
      "Epoch [372/500], Step [40/58], Loss: 1.6149\n",
      "Epoch [372/500], Step [50/58], Loss: 1.4624\n",
      "Epoch [373/500], Step [10/58], Loss: 2.0808\n",
      "Epoch [373/500], Step [20/58], Loss: 1.3211\n",
      "Epoch [373/500], Step [30/58], Loss: 1.7399\n",
      "Epoch [373/500], Step [40/58], Loss: 1.2853\n",
      "Epoch [373/500], Step [50/58], Loss: 1.1764\n",
      "Epoch [374/500], Step [10/58], Loss: 1.3751\n",
      "Epoch [374/500], Step [20/58], Loss: 1.3323\n",
      "Epoch [374/500], Step [30/58], Loss: 1.3421\n",
      "Epoch [374/500], Step [40/58], Loss: 1.3180\n",
      "Epoch [374/500], Step [50/58], Loss: 1.2379\n",
      "Epoch [375/500], Step [10/58], Loss: 1.3098\n",
      "Epoch [375/500], Step [20/58], Loss: 1.3158\n",
      "Epoch [375/500], Step [30/58], Loss: 1.2146\n",
      "Epoch [375/500], Step [40/58], Loss: 1.4637\n",
      "Epoch [375/500], Step [50/58], Loss: 1.5510\n",
      "Epoch [376/500], Step [10/58], Loss: 1.5716\n",
      "Epoch [376/500], Step [20/58], Loss: 1.1835\n",
      "Epoch [376/500], Step [30/58], Loss: 1.5689\n",
      "Epoch [376/500], Step [40/58], Loss: 1.7377\n",
      "Epoch [376/500], Step [50/58], Loss: 1.5877\n",
      "Epoch [377/500], Step [10/58], Loss: 1.5780\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [377/500], Step [20/58], Loss: 1.5402\n",
      "Epoch [377/500], Step [30/58], Loss: 1.4117\n",
      "Epoch [377/500], Step [40/58], Loss: 1.5113\n",
      "Epoch [377/500], Step [50/58], Loss: 1.4034\n",
      "Epoch [378/500], Step [10/58], Loss: 1.2835\n",
      "Epoch [378/500], Step [20/58], Loss: 1.6178\n",
      "Epoch [378/500], Step [30/58], Loss: 1.3087\n",
      "Epoch [378/500], Step [40/58], Loss: 1.2179\n",
      "Epoch [378/500], Step [50/58], Loss: 1.5742\n",
      "Epoch [379/500], Step [10/58], Loss: 1.4310\n",
      "Epoch [379/500], Step [20/58], Loss: 1.3561\n",
      "Epoch [379/500], Step [30/58], Loss: 1.6629\n",
      "Epoch [379/500], Step [40/58], Loss: 1.2257\n",
      "Epoch [379/500], Step [50/58], Loss: 1.2643\n",
      "Epoch [380/500], Step [10/58], Loss: 1.2062\n",
      "Epoch [380/500], Step [20/58], Loss: 1.3831\n",
      "Epoch [380/500], Step [30/58], Loss: 1.3078\n",
      "Epoch [380/500], Step [40/58], Loss: 1.3219\n",
      "Epoch [380/500], Step [50/58], Loss: 1.4138\n",
      "Epoch [381/500], Step [10/58], Loss: 1.1038\n",
      "Epoch [381/500], Step [20/58], Loss: 1.0866\n",
      "Epoch [381/500], Step [30/58], Loss: 1.1592\n",
      "Epoch [381/500], Step [40/58], Loss: 1.6080\n",
      "Epoch [381/500], Step [50/58], Loss: 1.6387\n",
      "Epoch [382/500], Step [10/58], Loss: 1.2463\n",
      "Epoch [382/500], Step [20/58], Loss: 1.3561\n",
      "Epoch [382/500], Step [30/58], Loss: 1.4499\n",
      "Epoch [382/500], Step [40/58], Loss: 1.1389\n",
      "Epoch [382/500], Step [50/58], Loss: 1.1752\n",
      "Epoch [383/500], Step [10/58], Loss: 1.2388\n",
      "Epoch [383/500], Step [20/58], Loss: 1.1590\n",
      "Epoch [383/500], Step [30/58], Loss: 1.7004\n",
      "Epoch [383/500], Step [40/58], Loss: 1.0678\n",
      "Epoch [383/500], Step [50/58], Loss: 1.3646\n",
      "Epoch [384/500], Step [10/58], Loss: 1.3197\n",
      "Epoch [384/500], Step [20/58], Loss: 1.4272\n",
      "Epoch [384/500], Step [30/58], Loss: 1.2907\n",
      "Epoch [384/500], Step [40/58], Loss: 1.2931\n",
      "Epoch [384/500], Step [50/58], Loss: 1.2982\n",
      "Epoch [385/500], Step [10/58], Loss: 1.0708\n",
      "Epoch [385/500], Step [20/58], Loss: 1.2766\n",
      "Epoch [385/500], Step [30/58], Loss: 1.2181\n",
      "Epoch [385/500], Step [40/58], Loss: 1.7367\n",
      "Epoch [385/500], Step [50/58], Loss: 1.4500\n",
      "Epoch [386/500], Step [10/58], Loss: 1.4868\n",
      "Epoch [386/500], Step [20/58], Loss: 1.3459\n",
      "Epoch [386/500], Step [30/58], Loss: 1.5231\n",
      "Epoch [386/500], Step [40/58], Loss: 1.3293\n",
      "Epoch [386/500], Step [50/58], Loss: 1.4399\n",
      "Epoch [387/500], Step [10/58], Loss: 1.2411\n",
      "Epoch [387/500], Step [20/58], Loss: 1.2852\n",
      "Epoch [387/500], Step [30/58], Loss: 1.4041\n",
      "Epoch [387/500], Step [40/58], Loss: 1.2995\n",
      "Epoch [387/500], Step [50/58], Loss: 1.2492\n",
      "Epoch [388/500], Step [10/58], Loss: 1.1456\n",
      "Epoch [388/500], Step [20/58], Loss: 1.1245\n",
      "Epoch [388/500], Step [30/58], Loss: 1.2873\n",
      "Epoch [388/500], Step [40/58], Loss: 1.1877\n",
      "Epoch [388/500], Step [50/58], Loss: 1.3200\n",
      "Epoch [389/500], Step [10/58], Loss: 1.2874\n",
      "Epoch [389/500], Step [20/58], Loss: 1.1257\n",
      "Epoch [389/500], Step [30/58], Loss: 1.1567\n",
      "Epoch [389/500], Step [40/58], Loss: 1.4612\n",
      "Epoch [389/500], Step [50/58], Loss: 1.0859\n",
      "Epoch [390/500], Step [10/58], Loss: 1.0944\n",
      "Epoch [390/500], Step [20/58], Loss: 1.3805\n",
      "Epoch [390/500], Step [30/58], Loss: 1.3234\n",
      "Epoch [390/500], Step [40/58], Loss: 1.1566\n",
      "Epoch [390/500], Step [50/58], Loss: 1.2513\n",
      "Epoch [391/500], Step [10/58], Loss: 1.2939\n",
      "Epoch [391/500], Step [20/58], Loss: 1.2195\n",
      "Epoch [391/500], Step [30/58], Loss: 1.4147\n",
      "Epoch [391/500], Step [40/58], Loss: 1.3124\n",
      "Epoch [391/500], Step [50/58], Loss: 1.2391\n",
      "Epoch [392/500], Step [10/58], Loss: 1.0360\n",
      "Epoch [392/500], Step [20/58], Loss: 1.4160\n",
      "Epoch [392/500], Step [30/58], Loss: 1.0619\n",
      "Epoch [392/500], Step [40/58], Loss: 1.4549\n",
      "Epoch [392/500], Step [50/58], Loss: 1.1761\n",
      "Epoch [393/500], Step [10/58], Loss: 1.2229\n",
      "Epoch [393/500], Step [20/58], Loss: 1.3147\n",
      "Epoch [393/500], Step [30/58], Loss: 0.9551\n",
      "Epoch [393/500], Step [40/58], Loss: 1.2935\n",
      "Epoch [393/500], Step [50/58], Loss: 1.5330\n",
      "Epoch [394/500], Step [10/58], Loss: 1.0753\n",
      "Epoch [394/500], Step [20/58], Loss: 1.4676\n",
      "Epoch [394/500], Step [30/58], Loss: 1.5026\n",
      "Epoch [394/500], Step [40/58], Loss: 1.2660\n",
      "Epoch [394/500], Step [50/58], Loss: 0.8078\n",
      "Epoch [395/500], Step [10/58], Loss: 1.3456\n",
      "Epoch [395/500], Step [20/58], Loss: 0.9037\n",
      "Epoch [395/500], Step [30/58], Loss: 1.1415\n",
      "Epoch [395/500], Step [40/58], Loss: 1.2437\n",
      "Epoch [395/500], Step [50/58], Loss: 1.1471\n",
      "Epoch [396/500], Step [10/58], Loss: 1.1520\n",
      "Epoch [396/500], Step [20/58], Loss: 1.2860\n",
      "Epoch [396/500], Step [30/58], Loss: 1.1321\n",
      "Epoch [396/500], Step [40/58], Loss: 0.9546\n",
      "Epoch [396/500], Step [50/58], Loss: 1.0691\n",
      "Epoch [397/500], Step [10/58], Loss: 0.9290\n",
      "Epoch [397/500], Step [20/58], Loss: 1.0342\n",
      "Epoch [397/500], Step [30/58], Loss: 1.2375\n",
      "Epoch [397/500], Step [40/58], Loss: 1.5738\n",
      "Epoch [397/500], Step [50/58], Loss: 1.1280\n",
      "Epoch [398/500], Step [10/58], Loss: 1.0981\n",
      "Epoch [398/500], Step [20/58], Loss: 1.4154\n",
      "Epoch [398/500], Step [30/58], Loss: 1.6441\n",
      "Epoch [398/500], Step [40/58], Loss: 1.3942\n",
      "Epoch [398/500], Step [50/58], Loss: 1.2917\n",
      "Epoch [399/500], Step [10/58], Loss: 1.2325\n",
      "Epoch [399/500], Step [20/58], Loss: 1.2345\n",
      "Epoch [399/500], Step [30/58], Loss: 1.2649\n",
      "Epoch [399/500], Step [40/58], Loss: 0.9919\n",
      "Epoch [399/500], Step [50/58], Loss: 1.2032\n",
      "Epoch [400/500], Step [10/58], Loss: 1.1311\n",
      "Epoch [400/500], Step [20/58], Loss: 1.3857\n",
      "Epoch [400/500], Step [30/58], Loss: 1.0399\n",
      "Epoch [400/500], Step [40/58], Loss: 1.1448\n",
      "Epoch [400/500], Step [50/58], Loss: 1.1814\n",
      "Epoch [401/500], Step [10/58], Loss: 0.9029\n",
      "Epoch [401/500], Step [20/58], Loss: 0.9453\n",
      "Epoch [401/500], Step [30/58], Loss: 1.3277\n",
      "Epoch [401/500], Step [40/58], Loss: 1.0806\n",
      "Epoch [401/500], Step [50/58], Loss: 3.0693\n",
      "Epoch [402/500], Step [10/58], Loss: 2.3019\n",
      "Epoch [402/500], Step [20/58], Loss: 2.2964\n",
      "Epoch [402/500], Step [30/58], Loss: 2.2017\n",
      "Epoch [402/500], Step [40/58], Loss: 2.2921\n",
      "Epoch [402/500], Step [50/58], Loss: 1.9778\n",
      "Epoch [403/500], Step [10/58], Loss: 2.1403\n",
      "Epoch [403/500], Step [20/58], Loss: 2.2179\n",
      "Epoch [403/500], Step [30/58], Loss: 2.2203\n",
      "Epoch [403/500], Step [40/58], Loss: 2.2388\n",
      "Epoch [403/500], Step [50/58], Loss: 2.3831\n",
      "Epoch [404/500], Step [10/58], Loss: 2.1444\n",
      "Epoch [404/500], Step [20/58], Loss: 2.0681\n",
      "Epoch [404/500], Step [30/58], Loss: 1.9071\n",
      "Epoch [404/500], Step [40/58], Loss: 2.0508\n",
      "Epoch [404/500], Step [50/58], Loss: 2.1636\n",
      "Epoch [405/500], Step [10/58], Loss: 2.0601\n",
      "Epoch [405/500], Step [20/58], Loss: 2.1977\n",
      "Epoch [405/500], Step [30/58], Loss: 1.8935\n",
      "Epoch [405/500], Step [40/58], Loss: 2.1066\n",
      "Epoch [405/500], Step [50/58], Loss: 2.0925\n",
      "Epoch [406/500], Step [10/58], Loss: 1.7353\n",
      "Epoch [406/500], Step [20/58], Loss: 1.7839\n",
      "Epoch [406/500], Step [30/58], Loss: 2.2252\n",
      "Epoch [406/500], Step [40/58], Loss: 2.0894\n",
      "Epoch [406/500], Step [50/58], Loss: 2.0607\n",
      "Epoch [407/500], Step [10/58], Loss: 1.9908\n",
      "Epoch [407/500], Step [20/58], Loss: 2.1123\n",
      "Epoch [407/500], Step [30/58], Loss: 1.9348\n",
      "Epoch [407/500], Step [40/58], Loss: 1.7506\n",
      "Epoch [407/500], Step [50/58], Loss: 1.8822\n",
      "Epoch [408/500], Step [10/58], Loss: 2.1650\n",
      "Epoch [408/500], Step [20/58], Loss: 1.9505\n",
      "Epoch [408/500], Step [30/58], Loss: 1.7148\n",
      "Epoch [408/500], Step [40/58], Loss: 1.7942\n",
      "Epoch [408/500], Step [50/58], Loss: 1.8858\n",
      "Epoch [409/500], Step [10/58], Loss: 2.1413\n",
      "Epoch [409/500], Step [20/58], Loss: 2.1552\n",
      "Epoch [409/500], Step [30/58], Loss: 2.1096\n",
      "Epoch [409/500], Step [40/58], Loss: 1.8763\n",
      "Epoch [409/500], Step [50/58], Loss: 1.7395\n",
      "Epoch [410/500], Step [10/58], Loss: 2.1775\n",
      "Epoch [410/500], Step [20/58], Loss: 1.9635\n",
      "Epoch [410/500], Step [30/58], Loss: 2.0449\n",
      "Epoch [410/500], Step [40/58], Loss: 1.9880\n",
      "Epoch [410/500], Step [50/58], Loss: 1.8448\n",
      "Epoch [411/500], Step [10/58], Loss: 2.0458\n",
      "Epoch [411/500], Step [20/58], Loss: 2.0024\n",
      "Epoch [411/500], Step [30/58], Loss: 1.9778\n",
      "Epoch [411/500], Step [40/58], Loss: 2.1476\n",
      "Epoch [411/500], Step [50/58], Loss: 1.8463\n",
      "Epoch [412/500], Step [10/58], Loss: 1.9274\n",
      "Epoch [412/500], Step [20/58], Loss: 1.7541\n",
      "Epoch [412/500], Step [30/58], Loss: 1.8768\n",
      "Epoch [412/500], Step [40/58], Loss: 2.0544\n",
      "Epoch [412/500], Step [50/58], Loss: 1.8015\n",
      "Epoch [413/500], Step [10/58], Loss: 1.8117\n",
      "Epoch [413/500], Step [20/58], Loss: 1.9720\n",
      "Epoch [413/500], Step [30/58], Loss: 1.7699\n",
      "Epoch [413/500], Step [40/58], Loss: 2.1256\n",
      "Epoch [413/500], Step [50/58], Loss: 2.0439\n",
      "Epoch [414/500], Step [10/58], Loss: 1.9447\n",
      "Epoch [414/500], Step [20/58], Loss: 2.0288\n",
      "Epoch [414/500], Step [30/58], Loss: 1.7105\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [414/500], Step [40/58], Loss: 1.9625\n",
      "Epoch [414/500], Step [50/58], Loss: 2.0192\n",
      "Epoch [415/500], Step [10/58], Loss: 1.7944\n",
      "Epoch [415/500], Step [20/58], Loss: 2.2082\n",
      "Epoch [415/500], Step [30/58], Loss: 2.2181\n",
      "Epoch [415/500], Step [40/58], Loss: 1.8942\n",
      "Epoch [415/500], Step [50/58], Loss: 1.6306\n",
      "Epoch [416/500], Step [10/58], Loss: 2.2199\n",
      "Epoch [416/500], Step [20/58], Loss: 1.7283\n",
      "Epoch [416/500], Step [30/58], Loss: 1.7957\n",
      "Epoch [416/500], Step [40/58], Loss: 1.6753\n",
      "Epoch [416/500], Step [50/58], Loss: 1.8672\n",
      "Epoch [417/500], Step [10/58], Loss: 2.1216\n",
      "Epoch [417/500], Step [20/58], Loss: 2.0478\n",
      "Epoch [417/500], Step [30/58], Loss: 1.6931\n",
      "Epoch [417/500], Step [40/58], Loss: 1.9910\n",
      "Epoch [417/500], Step [50/58], Loss: 1.6734\n",
      "Epoch [418/500], Step [10/58], Loss: 1.9772\n",
      "Epoch [418/500], Step [20/58], Loss: 1.8992\n",
      "Epoch [418/500], Step [30/58], Loss: 1.8157\n",
      "Epoch [418/500], Step [40/58], Loss: 2.0581\n",
      "Epoch [418/500], Step [50/58], Loss: 1.8397\n",
      "Epoch [419/500], Step [10/58], Loss: 1.8924\n",
      "Epoch [419/500], Step [20/58], Loss: 2.0164\n",
      "Epoch [419/500], Step [30/58], Loss: 1.8666\n",
      "Epoch [419/500], Step [40/58], Loss: 1.5733\n",
      "Epoch [419/500], Step [50/58], Loss: 1.6773\n",
      "Epoch [420/500], Step [10/58], Loss: 2.0109\n",
      "Epoch [420/500], Step [20/58], Loss: 1.7781\n",
      "Epoch [420/500], Step [30/58], Loss: 1.7484\n",
      "Epoch [420/500], Step [40/58], Loss: 1.5974\n",
      "Epoch [420/500], Step [50/58], Loss: 1.7461\n",
      "Epoch [421/500], Step [10/58], Loss: 2.2540\n",
      "Epoch [421/500], Step [20/58], Loss: 1.8559\n",
      "Epoch [421/500], Step [30/58], Loss: 1.9091\n",
      "Epoch [421/500], Step [40/58], Loss: 1.6800\n",
      "Epoch [421/500], Step [50/58], Loss: 1.7098\n",
      "Epoch [422/500], Step [10/58], Loss: 2.0093\n",
      "Epoch [422/500], Step [20/58], Loss: 2.0028\n",
      "Epoch [422/500], Step [30/58], Loss: 1.7035\n",
      "Epoch [422/500], Step [40/58], Loss: 1.6184\n",
      "Epoch [422/500], Step [50/58], Loss: 1.8702\n",
      "Epoch [423/500], Step [10/58], Loss: 1.8921\n",
      "Epoch [423/500], Step [20/58], Loss: 1.7583\n",
      "Epoch [423/500], Step [30/58], Loss: 1.7521\n",
      "Epoch [423/500], Step [40/58], Loss: 1.7127\n",
      "Epoch [423/500], Step [50/58], Loss: 1.6369\n",
      "Epoch [424/500], Step [10/58], Loss: 1.9231\n",
      "Epoch [424/500], Step [20/58], Loss: 1.6802\n",
      "Epoch [424/500], Step [30/58], Loss: 1.6437\n",
      "Epoch [424/500], Step [40/58], Loss: 1.8085\n",
      "Epoch [424/500], Step [50/58], Loss: 1.8918\n",
      "Epoch [425/500], Step [10/58], Loss: 1.7134\n",
      "Epoch [425/500], Step [20/58], Loss: 1.6452\n",
      "Epoch [425/500], Step [30/58], Loss: 1.7863\n",
      "Epoch [425/500], Step [40/58], Loss: 1.8360\n",
      "Epoch [425/500], Step [50/58], Loss: 1.5639\n",
      "Epoch [426/500], Step [10/58], Loss: 2.2145\n",
      "Epoch [426/500], Step [20/58], Loss: 1.6467\n",
      "Epoch [426/500], Step [30/58], Loss: 1.9099\n",
      "Epoch [426/500], Step [40/58], Loss: 1.8595\n",
      "Epoch [426/500], Step [50/58], Loss: 1.8216\n",
      "Epoch [427/500], Step [10/58], Loss: 1.8858\n",
      "Epoch [427/500], Step [20/58], Loss: 1.7477\n",
      "Epoch [427/500], Step [30/58], Loss: 1.5368\n",
      "Epoch [427/500], Step [40/58], Loss: 1.6967\n",
      "Epoch [427/500], Step [50/58], Loss: 1.7778\n",
      "Epoch [428/500], Step [10/58], Loss: 1.6680\n",
      "Epoch [428/500], Step [20/58], Loss: 1.8852\n",
      "Epoch [428/500], Step [30/58], Loss: 1.5965\n",
      "Epoch [428/500], Step [40/58], Loss: 1.6925\n",
      "Epoch [428/500], Step [50/58], Loss: 1.7084\n",
      "Epoch [429/500], Step [10/58], Loss: 1.7953\n",
      "Epoch [429/500], Step [20/58], Loss: 1.6557\n",
      "Epoch [429/500], Step [30/58], Loss: 2.0314\n",
      "Epoch [429/500], Step [40/58], Loss: 1.5908\n",
      "Epoch [429/500], Step [50/58], Loss: 1.6871\n",
      "Epoch [430/500], Step [10/58], Loss: 2.1162\n",
      "Epoch [430/500], Step [20/58], Loss: 1.9193\n",
      "Epoch [430/500], Step [30/58], Loss: 2.0385\n",
      "Epoch [430/500], Step [40/58], Loss: 1.9331\n",
      "Epoch [430/500], Step [50/58], Loss: 1.6234\n",
      "Epoch [431/500], Step [10/58], Loss: 1.8123\n",
      "Epoch [431/500], Step [20/58], Loss: 1.8038\n",
      "Epoch [431/500], Step [30/58], Loss: 1.8830\n",
      "Epoch [431/500], Step [40/58], Loss: 2.0677\n",
      "Epoch [431/500], Step [50/58], Loss: 1.5624\n",
      "Epoch [432/500], Step [10/58], Loss: 1.6167\n",
      "Epoch [432/500], Step [20/58], Loss: 1.7746\n",
      "Epoch [432/500], Step [30/58], Loss: 1.8598\n",
      "Epoch [432/500], Step [40/58], Loss: 1.3369\n",
      "Epoch [432/500], Step [50/58], Loss: 1.8759\n",
      "Epoch [433/500], Step [10/58], Loss: 1.6446\n",
      "Epoch [433/500], Step [20/58], Loss: 1.8131\n",
      "Epoch [433/500], Step [30/58], Loss: 1.5334\n",
      "Epoch [433/500], Step [40/58], Loss: 1.8349\n",
      "Epoch [433/500], Step [50/58], Loss: 1.6794\n",
      "Epoch [434/500], Step [10/58], Loss: 1.4675\n",
      "Epoch [434/500], Step [20/58], Loss: 1.6479\n",
      "Epoch [434/500], Step [30/58], Loss: 1.4890\n",
      "Epoch [434/500], Step [40/58], Loss: 1.5462\n",
      "Epoch [434/500], Step [50/58], Loss: 1.6644\n",
      "Epoch [435/500], Step [10/58], Loss: 1.5302\n",
      "Epoch [435/500], Step [20/58], Loss: 1.4622\n",
      "Epoch [435/500], Step [30/58], Loss: 1.4467\n",
      "Epoch [435/500], Step [40/58], Loss: 1.7022\n",
      "Epoch [435/500], Step [50/58], Loss: 1.6672\n",
      "Epoch [436/500], Step [10/58], Loss: 1.6425\n",
      "Epoch [436/500], Step [20/58], Loss: 1.9887\n",
      "Epoch [436/500], Step [30/58], Loss: 1.5240\n",
      "Epoch [436/500], Step [40/58], Loss: 2.1042\n",
      "Epoch [436/500], Step [50/58], Loss: 1.8591\n",
      "Epoch [437/500], Step [10/58], Loss: 1.8472\n",
      "Epoch [437/500], Step [20/58], Loss: 1.6387\n",
      "Epoch [437/500], Step [30/58], Loss: 1.7153\n",
      "Epoch [437/500], Step [40/58], Loss: 1.2850\n",
      "Epoch [437/500], Step [50/58], Loss: 2.0201\n",
      "Epoch [438/500], Step [10/58], Loss: 1.6247\n",
      "Epoch [438/500], Step [20/58], Loss: 1.5661\n",
      "Epoch [438/500], Step [30/58], Loss: 1.5889\n",
      "Epoch [438/500], Step [40/58], Loss: 1.4984\n",
      "Epoch [438/500], Step [50/58], Loss: 1.6672\n",
      "Epoch [439/500], Step [10/58], Loss: 1.9052\n",
      "Epoch [439/500], Step [20/58], Loss: 1.5297\n",
      "Epoch [439/500], Step [30/58], Loss: 1.5517\n",
      "Epoch [439/500], Step [40/58], Loss: 1.6625\n",
      "Epoch [439/500], Step [50/58], Loss: 1.4358\n",
      "Epoch [440/500], Step [10/58], Loss: 1.2420\n",
      "Epoch [440/500], Step [20/58], Loss: 1.5382\n",
      "Epoch [440/500], Step [30/58], Loss: 1.1946\n",
      "Epoch [440/500], Step [40/58], Loss: 1.4487\n",
      "Epoch [440/500], Step [50/58], Loss: 1.5549\n",
      "Epoch [441/500], Step [10/58], Loss: 1.5683\n",
      "Epoch [441/500], Step [20/58], Loss: 1.5915\n",
      "Epoch [441/500], Step [30/58], Loss: 1.4983\n",
      "Epoch [441/500], Step [40/58], Loss: 1.5220\n",
      "Epoch [441/500], Step [50/58], Loss: 1.6466\n",
      "Epoch [442/500], Step [10/58], Loss: 1.8031\n",
      "Epoch [442/500], Step [20/58], Loss: 1.3862\n",
      "Epoch [442/500], Step [30/58], Loss: 1.6648\n",
      "Epoch [442/500], Step [40/58], Loss: 1.9672\n",
      "Epoch [442/500], Step [50/58], Loss: 1.2686\n",
      "Epoch [443/500], Step [10/58], Loss: 2.0514\n",
      "Epoch [443/500], Step [20/58], Loss: 1.2090\n",
      "Epoch [443/500], Step [30/58], Loss: 1.7451\n",
      "Epoch [443/500], Step [40/58], Loss: 1.6085\n",
      "Epoch [443/500], Step [50/58], Loss: 1.5215\n",
      "Epoch [444/500], Step [10/58], Loss: 1.5239\n",
      "Epoch [444/500], Step [20/58], Loss: 1.6586\n",
      "Epoch [444/500], Step [30/58], Loss: 1.4783\n",
      "Epoch [444/500], Step [40/58], Loss: 1.3425\n",
      "Epoch [444/500], Step [50/58], Loss: 1.6122\n",
      "Epoch [445/500], Step [10/58], Loss: 1.5314\n",
      "Epoch [445/500], Step [20/58], Loss: 1.4673\n",
      "Epoch [445/500], Step [30/58], Loss: 1.2722\n",
      "Epoch [445/500], Step [40/58], Loss: 1.5227\n",
      "Epoch [445/500], Step [50/58], Loss: 1.1775\n",
      "Epoch [446/500], Step [10/58], Loss: 1.4105\n",
      "Epoch [446/500], Step [20/58], Loss: 1.3356\n",
      "Epoch [446/500], Step [30/58], Loss: 1.1278\n",
      "Epoch [446/500], Step [40/58], Loss: 1.2861\n",
      "Epoch [446/500], Step [50/58], Loss: 1.5370\n",
      "Epoch [447/500], Step [10/58], Loss: 1.3243\n",
      "Epoch [447/500], Step [20/58], Loss: 1.0895\n",
      "Epoch [447/500], Step [30/58], Loss: 1.3838\n",
      "Epoch [447/500], Step [40/58], Loss: 1.3274\n",
      "Epoch [447/500], Step [50/58], Loss: 1.4181\n",
      "Epoch [448/500], Step [10/58], Loss: 1.5774\n",
      "Epoch [448/500], Step [20/58], Loss: 1.3057\n",
      "Epoch [448/500], Step [30/58], Loss: 1.2833\n",
      "Epoch [448/500], Step [40/58], Loss: 1.6868\n",
      "Epoch [448/500], Step [50/58], Loss: 1.3513\n",
      "Epoch [449/500], Step [10/58], Loss: 1.2583\n",
      "Epoch [449/500], Step [20/58], Loss: 1.2296\n",
      "Epoch [449/500], Step [30/58], Loss: 1.4699\n",
      "Epoch [449/500], Step [40/58], Loss: 1.4314\n",
      "Epoch [449/500], Step [50/58], Loss: 1.5809\n",
      "Epoch [450/500], Step [10/58], Loss: 1.3105\n",
      "Epoch [450/500], Step [20/58], Loss: 1.3385\n",
      "Epoch [450/500], Step [30/58], Loss: 1.2020\n",
      "Epoch [450/500], Step [40/58], Loss: 0.8762\n",
      "Epoch [450/500], Step [50/58], Loss: 1.1604\n",
      "Epoch [451/500], Step [10/58], Loss: 1.3080\n",
      "Epoch [451/500], Step [20/58], Loss: 1.3447\n",
      "Epoch [451/500], Step [30/58], Loss: 1.3686\n",
      "Epoch [451/500], Step [40/58], Loss: 1.4770\n",
      "Epoch [451/500], Step [50/58], Loss: 1.2050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [452/500], Step [10/58], Loss: 1.2627\n",
      "Epoch [452/500], Step [20/58], Loss: 1.2659\n",
      "Epoch [452/500], Step [30/58], Loss: 1.0813\n",
      "Epoch [452/500], Step [40/58], Loss: 2.0934\n",
      "Epoch [452/500], Step [50/58], Loss: 1.2087\n",
      "Epoch [453/500], Step [10/58], Loss: 1.6161\n",
      "Epoch [453/500], Step [20/58], Loss: 1.4842\n",
      "Epoch [453/500], Step [30/58], Loss: 1.3937\n",
      "Epoch [453/500], Step [40/58], Loss: 1.7620\n",
      "Epoch [453/500], Step [50/58], Loss: 1.6846\n",
      "Epoch [454/500], Step [10/58], Loss: 1.0530\n",
      "Epoch [454/500], Step [20/58], Loss: 1.1284\n",
      "Epoch [454/500], Step [30/58], Loss: 1.2231\n",
      "Epoch [454/500], Step [40/58], Loss: 1.4038\n",
      "Epoch [454/500], Step [50/58], Loss: 1.2735\n",
      "Epoch [455/500], Step [10/58], Loss: 1.5949\n",
      "Epoch [455/500], Step [20/58], Loss: 1.1263\n",
      "Epoch [455/500], Step [30/58], Loss: 1.0049\n",
      "Epoch [455/500], Step [40/58], Loss: 1.1471\n",
      "Epoch [455/500], Step [50/58], Loss: 1.3829\n",
      "Epoch [456/500], Step [10/58], Loss: 1.3138\n",
      "Epoch [456/500], Step [20/58], Loss: 1.6860\n",
      "Epoch [456/500], Step [30/58], Loss: 1.5971\n",
      "Epoch [456/500], Step [40/58], Loss: 1.3449\n",
      "Epoch [456/500], Step [50/58], Loss: 1.0290\n",
      "Epoch [457/500], Step [10/58], Loss: 1.3387\n",
      "Epoch [457/500], Step [20/58], Loss: 1.0783\n",
      "Epoch [457/500], Step [30/58], Loss: 1.3035\n",
      "Epoch [457/500], Step [40/58], Loss: 1.1617\n",
      "Epoch [457/500], Step [50/58], Loss: 1.4321\n",
      "Epoch [458/500], Step [10/58], Loss: 0.8680\n",
      "Epoch [458/500], Step [20/58], Loss: 1.2126\n",
      "Epoch [458/500], Step [30/58], Loss: 1.2886\n",
      "Epoch [458/500], Step [40/58], Loss: 1.4757\n",
      "Epoch [458/500], Step [50/58], Loss: 1.3982\n",
      "Epoch [459/500], Step [10/58], Loss: 1.1648\n",
      "Epoch [459/500], Step [20/58], Loss: 1.4805\n",
      "Epoch [459/500], Step [30/58], Loss: 1.4718\n",
      "Epoch [459/500], Step [40/58], Loss: 1.2053\n",
      "Epoch [459/500], Step [50/58], Loss: 1.0068\n",
      "Epoch [460/500], Step [10/58], Loss: 0.9164\n",
      "Epoch [460/500], Step [20/58], Loss: 1.1517\n",
      "Epoch [460/500], Step [30/58], Loss: 1.4099\n",
      "Epoch [460/500], Step [40/58], Loss: 1.0367\n",
      "Epoch [460/500], Step [50/58], Loss: 1.2671\n",
      "Epoch [461/500], Step [10/58], Loss: 1.5246\n",
      "Epoch [461/500], Step [20/58], Loss: 1.2577\n",
      "Epoch [461/500], Step [30/58], Loss: 1.4251\n",
      "Epoch [461/500], Step [40/58], Loss: 1.1159\n",
      "Epoch [461/500], Step [50/58], Loss: 0.9183\n",
      "Epoch [462/500], Step [10/58], Loss: 1.1082\n",
      "Epoch [462/500], Step [20/58], Loss: 1.2493\n",
      "Epoch [462/500], Step [30/58], Loss: 1.4027\n",
      "Epoch [462/500], Step [40/58], Loss: 1.3962\n",
      "Epoch [462/500], Step [50/58], Loss: 1.2879\n",
      "Epoch [463/500], Step [10/58], Loss: 0.9270\n",
      "Epoch [463/500], Step [20/58], Loss: 1.0305\n",
      "Epoch [463/500], Step [30/58], Loss: 1.0616\n",
      "Epoch [463/500], Step [40/58], Loss: 0.9736\n",
      "Epoch [463/500], Step [50/58], Loss: 1.1291\n",
      "Epoch [464/500], Step [10/58], Loss: 0.9874\n",
      "Epoch [464/500], Step [20/58], Loss: 1.0609\n",
      "Epoch [464/500], Step [30/58], Loss: 1.1909\n",
      "Epoch [464/500], Step [40/58], Loss: 1.1853\n",
      "Epoch [464/500], Step [50/58], Loss: 1.2542\n",
      "Epoch [465/500], Step [10/58], Loss: 0.9085\n",
      "Epoch [465/500], Step [20/58], Loss: 0.9047\n",
      "Epoch [465/500], Step [30/58], Loss: 1.1643\n",
      "Epoch [465/500], Step [40/58], Loss: 1.0491\n",
      "Epoch [465/500], Step [50/58], Loss: 0.8046\n",
      "Epoch [466/500], Step [10/58], Loss: 1.0740\n",
      "Epoch [466/500], Step [20/58], Loss: 1.3651\n",
      "Epoch [466/500], Step [30/58], Loss: 0.9076\n",
      "Epoch [466/500], Step [40/58], Loss: 1.1352\n",
      "Epoch [466/500], Step [50/58], Loss: 1.2650\n",
      "Epoch [467/500], Step [10/58], Loss: 1.0281\n",
      "Epoch [467/500], Step [20/58], Loss: 1.0362\n",
      "Epoch [467/500], Step [30/58], Loss: 1.2486\n",
      "Epoch [467/500], Step [40/58], Loss: 1.2291\n",
      "Epoch [467/500], Step [50/58], Loss: 1.5096\n",
      "Epoch [468/500], Step [10/58], Loss: 1.1694\n",
      "Epoch [468/500], Step [20/58], Loss: 1.1238\n",
      "Epoch [468/500], Step [30/58], Loss: 0.8906\n",
      "Epoch [468/500], Step [40/58], Loss: 1.0353\n",
      "Epoch [468/500], Step [50/58], Loss: 0.9941\n",
      "Epoch [469/500], Step [10/58], Loss: 1.1309\n",
      "Epoch [469/500], Step [20/58], Loss: 1.1236\n",
      "Epoch [469/500], Step [30/58], Loss: 1.2376\n",
      "Epoch [469/500], Step [40/58], Loss: 0.8733\n",
      "Epoch [469/500], Step [50/58], Loss: 1.3004\n",
      "Epoch [470/500], Step [10/58], Loss: 1.2040\n",
      "Epoch [470/500], Step [20/58], Loss: 0.8227\n",
      "Epoch [470/500], Step [30/58], Loss: 0.9728\n",
      "Epoch [470/500], Step [40/58], Loss: 1.1505\n",
      "Epoch [470/500], Step [50/58], Loss: 1.2229\n",
      "Epoch [471/500], Step [10/58], Loss: 1.2000\n",
      "Epoch [471/500], Step [20/58], Loss: 0.8480\n",
      "Epoch [471/500], Step [30/58], Loss: 0.8237\n",
      "Epoch [471/500], Step [40/58], Loss: 0.8559\n",
      "Epoch [471/500], Step [50/58], Loss: 0.9089\n",
      "Epoch [472/500], Step [10/58], Loss: 1.2237\n",
      "Epoch [472/500], Step [20/58], Loss: 1.2651\n",
      "Epoch [472/500], Step [30/58], Loss: 1.1891\n",
      "Epoch [472/500], Step [40/58], Loss: 1.0058\n",
      "Epoch [472/500], Step [50/58], Loss: 0.9436\n",
      "Epoch [473/500], Step [10/58], Loss: 0.8982\n",
      "Epoch [473/500], Step [20/58], Loss: 1.0979\n",
      "Epoch [473/500], Step [30/58], Loss: 0.9210\n",
      "Epoch [473/500], Step [40/58], Loss: 0.8974\n",
      "Epoch [473/500], Step [50/58], Loss: 1.5673\n",
      "Epoch [474/500], Step [10/58], Loss: 0.8692\n",
      "Epoch [474/500], Step [20/58], Loss: 1.0078\n",
      "Epoch [474/500], Step [30/58], Loss: 0.9692\n",
      "Epoch [474/500], Step [40/58], Loss: 0.7561\n",
      "Epoch [474/500], Step [50/58], Loss: 0.9931\n",
      "Epoch [475/500], Step [10/58], Loss: 1.0846\n",
      "Epoch [475/500], Step [20/58], Loss: 0.9467\n",
      "Epoch [475/500], Step [30/58], Loss: 0.9393\n",
      "Epoch [475/500], Step [40/58], Loss: 1.1059\n",
      "Epoch [475/500], Step [50/58], Loss: 1.0093\n",
      "Epoch [476/500], Step [10/58], Loss: 1.0774\n",
      "Epoch [476/500], Step [20/58], Loss: 1.0928\n",
      "Epoch [476/500], Step [30/58], Loss: 0.9723\n",
      "Epoch [476/500], Step [40/58], Loss: 1.3838\n",
      "Epoch [476/500], Step [50/58], Loss: 0.9001\n",
      "Epoch [477/500], Step [10/58], Loss: 0.9067\n",
      "Epoch [477/500], Step [20/58], Loss: 0.9988\n",
      "Epoch [477/500], Step [30/58], Loss: 0.8991\n",
      "Epoch [477/500], Step [40/58], Loss: 0.9443\n",
      "Epoch [477/500], Step [50/58], Loss: 0.9820\n",
      "Epoch [478/500], Step [10/58], Loss: 1.1633\n",
      "Epoch [478/500], Step [20/58], Loss: 0.9206\n",
      "Epoch [478/500], Step [30/58], Loss: 0.8999\n",
      "Epoch [478/500], Step [40/58], Loss: 0.8912\n",
      "Epoch [478/500], Step [50/58], Loss: 1.0973\n",
      "Epoch [479/500], Step [10/58], Loss: 0.7151\n",
      "Epoch [479/500], Step [20/58], Loss: 0.8278\n",
      "Epoch [479/500], Step [30/58], Loss: 0.9399\n",
      "Epoch [479/500], Step [40/58], Loss: 1.2624\n",
      "Epoch [479/500], Step [50/58], Loss: 1.1231\n",
      "Epoch [480/500], Step [10/58], Loss: 1.2222\n",
      "Epoch [480/500], Step [20/58], Loss: 0.7003\n",
      "Epoch [480/500], Step [30/58], Loss: 1.1741\n",
      "Epoch [480/500], Step [40/58], Loss: 1.2059\n",
      "Epoch [480/500], Step [50/58], Loss: 0.5954\n",
      "Epoch [481/500], Step [10/58], Loss: 0.6446\n",
      "Epoch [481/500], Step [20/58], Loss: 0.8274\n",
      "Epoch [481/500], Step [30/58], Loss: 0.7747\n",
      "Epoch [481/500], Step [40/58], Loss: 0.6873\n",
      "Epoch [481/500], Step [50/58], Loss: 0.9039\n",
      "Epoch [482/500], Step [10/58], Loss: 0.8789\n",
      "Epoch [482/500], Step [20/58], Loss: 1.1068\n",
      "Epoch [482/500], Step [30/58], Loss: 0.7987\n",
      "Epoch [482/500], Step [40/58], Loss: 1.2645\n",
      "Epoch [482/500], Step [50/58], Loss: 0.9883\n",
      "Epoch [483/500], Step [10/58], Loss: 1.3148\n",
      "Epoch [483/500], Step [20/58], Loss: 0.9878\n",
      "Epoch [483/500], Step [30/58], Loss: 1.0990\n",
      "Epoch [483/500], Step [40/58], Loss: 1.1016\n",
      "Epoch [483/500], Step [50/58], Loss: 1.2773\n",
      "Epoch [484/500], Step [10/58], Loss: 1.0132\n",
      "Epoch [484/500], Step [20/58], Loss: 0.9248\n",
      "Epoch [484/500], Step [30/58], Loss: 1.1898\n",
      "Epoch [484/500], Step [40/58], Loss: 1.0949\n",
      "Epoch [484/500], Step [50/58], Loss: 0.8713\n",
      "Epoch [485/500], Step [10/58], Loss: 0.8562\n",
      "Epoch [485/500], Step [20/58], Loss: 0.9395\n",
      "Epoch [485/500], Step [30/58], Loss: 1.0049\n",
      "Epoch [485/500], Step [40/58], Loss: 1.0490\n",
      "Epoch [485/500], Step [50/58], Loss: 1.1795\n",
      "Epoch [486/500], Step [10/58], Loss: 0.9702\n",
      "Epoch [486/500], Step [20/58], Loss: 1.0109\n",
      "Epoch [486/500], Step [30/58], Loss: 1.0234\n",
      "Epoch [486/500], Step [40/58], Loss: 0.9346\n",
      "Epoch [486/500], Step [50/58], Loss: 1.4697\n",
      "Epoch [487/500], Step [10/58], Loss: 1.1489\n",
      "Epoch [487/500], Step [20/58], Loss: 0.9087\n",
      "Epoch [487/500], Step [30/58], Loss: 0.6130\n",
      "Epoch [487/500], Step [40/58], Loss: 0.7336\n",
      "Epoch [487/500], Step [50/58], Loss: 0.9578\n",
      "Epoch [488/500], Step [10/58], Loss: 1.0122\n",
      "Epoch [488/500], Step [20/58], Loss: 0.8691\n",
      "Epoch [488/500], Step [30/58], Loss: 0.9151\n",
      "Epoch [488/500], Step [40/58], Loss: 0.7760\n",
      "Epoch [488/500], Step [50/58], Loss: 1.3283\n",
      "Epoch [489/500], Step [10/58], Loss: 0.8641\n",
      "Epoch [489/500], Step [20/58], Loss: 0.9198\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [489/500], Step [30/58], Loss: 0.7267\n",
      "Epoch [489/500], Step [40/58], Loss: 0.9047\n",
      "Epoch [489/500], Step [50/58], Loss: 1.0970\n",
      "Epoch [490/500], Step [10/58], Loss: 1.0892\n",
      "Epoch [490/500], Step [20/58], Loss: 0.8585\n",
      "Epoch [490/500], Step [30/58], Loss: 1.0324\n",
      "Epoch [490/500], Step [40/58], Loss: 1.2001\n",
      "Epoch [490/500], Step [50/58], Loss: 0.9171\n",
      "Epoch [491/500], Step [10/58], Loss: 0.9132\n",
      "Epoch [491/500], Step [20/58], Loss: 0.7239\n",
      "Epoch [491/500], Step [30/58], Loss: 1.0469\n",
      "Epoch [491/500], Step [40/58], Loss: 0.6968\n",
      "Epoch [491/500], Step [50/58], Loss: 1.0072\n",
      "Epoch [492/500], Step [10/58], Loss: 1.0538\n",
      "Epoch [492/500], Step [20/58], Loss: 0.9694\n",
      "Epoch [492/500], Step [30/58], Loss: 0.9108\n",
      "Epoch [492/500], Step [40/58], Loss: 1.6160\n",
      "Epoch [492/500], Step [50/58], Loss: 1.0361\n",
      "Epoch [493/500], Step [10/58], Loss: 1.0818\n",
      "Epoch [493/500], Step [20/58], Loss: 0.8468\n",
      "Epoch [493/500], Step [30/58], Loss: 0.9523\n",
      "Epoch [493/500], Step [40/58], Loss: 1.1072\n",
      "Epoch [493/500], Step [50/58], Loss: 0.9925\n",
      "Epoch [494/500], Step [10/58], Loss: 0.8684\n",
      "Epoch [494/500], Step [20/58], Loss: 1.0111\n",
      "Epoch [494/500], Step [30/58], Loss: 0.7570\n",
      "Epoch [494/500], Step [40/58], Loss: 0.7476\n",
      "Epoch [494/500], Step [50/58], Loss: 1.1668\n",
      "Epoch [495/500], Step [10/58], Loss: 1.1972\n",
      "Epoch [495/500], Step [20/58], Loss: 0.6713\n",
      "Epoch [495/500], Step [30/58], Loss: 0.7481\n",
      "Epoch [495/500], Step [40/58], Loss: 0.5769\n",
      "Epoch [495/500], Step [50/58], Loss: 0.8842\n",
      "Epoch [496/500], Step [10/58], Loss: 0.9506\n",
      "Epoch [496/500], Step [20/58], Loss: 0.5901\n",
      "Epoch [496/500], Step [30/58], Loss: 0.7095\n",
      "Epoch [496/500], Step [40/58], Loss: 0.9468\n",
      "Epoch [496/500], Step [50/58], Loss: 0.5837\n",
      "Epoch [497/500], Step [10/58], Loss: 0.7881\n",
      "Epoch [497/500], Step [20/58], Loss: 0.9522\n",
      "Epoch [497/500], Step [30/58], Loss: 1.1043\n",
      "Epoch [497/500], Step [40/58], Loss: 0.8647\n",
      "Epoch [497/500], Step [50/58], Loss: 0.7574\n",
      "Epoch [498/500], Step [10/58], Loss: 1.1751\n",
      "Epoch [498/500], Step [20/58], Loss: 1.1600\n",
      "Epoch [498/500], Step [30/58], Loss: 0.7105\n",
      "Epoch [498/500], Step [40/58], Loss: 0.7214\n",
      "Epoch [498/500], Step [50/58], Loss: 1.0552\n",
      "Epoch [499/500], Step [10/58], Loss: 0.6839\n",
      "Epoch [499/500], Step [20/58], Loss: 0.7067\n",
      "Epoch [499/500], Step [30/58], Loss: 1.0436\n",
      "Epoch [499/500], Step [40/58], Loss: 0.8395\n",
      "Epoch [499/500], Step [50/58], Loss: 0.9163\n",
      "Epoch [500/500], Step [10/58], Loss: 0.9970\n",
      "Epoch [500/500], Step [20/58], Loss: 0.8067\n",
      "Epoch [500/500], Step [30/58], Loss: 0.7971\n",
      "Epoch [500/500], Step [40/58], Loss: 1.0390\n",
      "Epoch [500/500], Step [50/58], Loss: 0.7863\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "#         print(i, images.shape, labels.shape, labels)\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        images = images.reshape(-1, 300, input_size).to(device)\n",
    "#         labels = labels.to(device)\n",
    "        #print(images.shape, labels.shape)\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 10 == 0: #to high to see the step... 100 number of  58\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6. Evaluate the model on test data. In this part, it is expected to choose appropriate evaluation metrics based on your task. For an instance, for classification task, accuracy, precision and recall should be computed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the model on the 10000 test images: 65.05 %\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'precision_score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-129-eef72fdc0e19>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mcorrect\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpredicted\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Test Accuracy of the model on the 10000 test images: {} %'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mcorrect\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Precision: {} %'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprecision_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpredicted\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'weighted'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Recall: {} %'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecall_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpredicted\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'weighted'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'precision_score' is not defined"
     ]
    }
   ],
   "source": [
    "#test the model\n",
    "\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.reshape(-1, 300, input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    print('Test Accuracy of the model on the 10000 test images: {} %'.format(round((100 * correct / total), 2))) \n",
    "    print('Precision: {} %'.format(round(torch.tensor(precision_score(labels,predicted, average='weighted')).item()*100, 2)))\n",
    "    print('Recall: {} %'.format(round(torch.tensor(recall_score(labels,predicted, average='weighted')).item()*100, 2)))\n",
    "    \n",
    "\n",
    " \n",
    "\n",
    "# Save the model checkpoint\n",
    "torch.save(model.state_dict(), 'model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 100, 100])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAArCAYAAABozHPFAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAJOElEQVR4nO2dbYgdVxnHf/+5d+9udxObbCO1NtVGLC2hoC2hVipSqh+qBiviS0SliFIQxVosUvuhqNAPBbEtKEqpShHxhbRgkGCpbdF+kGDTFGoSi6GaJjU1rXnPZvfuvffxw0wyZ05m7s6G7d703ucHy87Mec6Z55x55txznnPmHJkZjuM4zpufZNAKOI7jOEuDV+iO4zhDglfojuM4Q4JX6I7jOEOCV+iO4zhDglfojuM4Q0KtCl3SzZJelLRH0l0l4eOSfpuFb5N0+ZJr6jiO4/RlwQpdUgP4MfARYD3wOUnrI7EvA4fN7N3A/cB9S62o4ziO059mDZnrgD1m9hKApN8AtwC7AplbgO9mx5uBH0mS9flqqTkxZa2V0+mJosDg3FR+faGwyvTC6/HPWSEN6xPW51aNxX+oJVXHSarCIn2SWN+KtFWRD0Xxw/s21CuEWVDwYfrNSC5M0+oW4DnQs6VNuxfpGuY3DgvvHeZxKb7X61pgoDXT61cWYT5q6xen1y9ezeeQtIOTwGRiUw/NqWBakfKR2QVyUXrhSS8PVC8SrCqcWK6QeJT3qqKIk+h2Su9rvShTQbzjHH7dzN5alnydCv1SYF9wvh94X5WMmXUkHQUuAl4PhSTdBtwGMLZiNVd98g4Aeo1I92ZeGt3x4Hok122FcfLjXlSphmHhO9Ibj+WCAo0r5qqKOro+NjlfLheRNPIH1mzmx42k+CBbzW5p/GajeH28US43dpZcp1SulRSvTwRyq1qnCmGnumNnji9o5Pld1Zwp3itIc65Xx9TOjZlea2GhRdCOdA3zO9cths0G52G8WO5cONHO89XpNvpI5rQ7RbluLzf4Tic/nm/X08+60Y9bkAZRmNr1huSmXs51TOby64254rvUCMLGj+XvRdKN5GbL381kPmpgBPEas7ltJqeK76zmy98lnZorvQ5gY1F5NiueVydK+9CR/Lid69GbKb5L1sn1/ZNt3lulx7IOiprZQ2a2wcw2NCemlvPWjuM4Q0+dn+lXgMuC87XZtTKZ/ZKawIXA//olagnMT6a/8BZpEbbEq1rhUGyJV7XC07Cw5R3EH4+6NWFrO2p5q1ner4tdLK3x8hZwTNgSD1vhy9ny7he/X8u7leQPZTLoP68eO1m8l/I0Z+OHt4SMd4u6TzaqW1IhM2H3LyDuTYT5bTeq89GvJX8ujDcmFp1e2KqHYsu+ndRr5Yf0usWXqRu+c3HrvSqR6B3pTAYt9EClsGeenufHSaBH0immF/fI8zjF9JL5sB7IwxqNYh7VLX/Xk7H65WfN8rayoh5UsnJFfhK03pPWWEEubL1ztPq+dVrofwOukLROUgvYBGyJZLYAt2bHnwKe6uc/dxzHcZaeBX/2M5/414HHgQbwczPbKen7wLNmtgX4GfBLSXuAQ6SVvuM4jrOM1OrHmdlWYGt07Z7geBb49GJu3Js0jl87C5zttmgEA4ahCyMeMAwHfKpcGFB0Y4QujBWtYtc8dE/Erokql0bomoCz3RNVhAOGoYsgdFMATCbl7oOJpHjfqSo5FeUq04vkpgI9ppNiec4Ej2sy6NVON4oujHEFLgirN1h8Lsz0imkfimcIVDCdlHdQZ60YP8zvbDQyv6+zKgjL83uyV+7OWQyHOnl3vK7LKrafMF7oYjrcmayV3mIGiI/NT1DGiXaxLPayJrhB4EqZKz6PZC43rrHjwcyqyJXSaFNKVBSEZhwOwMYeutilUxYnZn6qqHs80aNMB4DJg/kzDgdxG7NFwYIb6K+VatSah36ZpKcl7ZK0U9LtJTI3Sjoq6fns756ytBzHcZw3jjo/+x3gW2b2nKSVwHZJT5jZrkjuGTPbuPQqOo7jOHVYsIVuZgfM7Lns+Diwm3TeueM4jnMeocVMRsnWaPkLcLWZHQuu3wg8SvrR0X+AO81sZ0n8Mx8WAVcCLwJriD5AGmG8LHK8LHK8LHK8LOCdVV+K1q7QJa0A/gzca2aPRWFvAXpmdkLSR4EHzeyKmuk+a2Ybaikx5HhZ5HhZ5HhZ5HhZ9KfuaotjpC3wX8WVOYCZHTOzE9nxVmBM0ppYznEcx3njqDPLRaTzzHeb2Q8rZN6WySHpuizdvl+KOo7jOEtLnVkuNwBfBF6Q9Hx27W7gHQBm9lPSr0O/KqkDnAI2LeJL0YcWpfFw42WR42WR42WR42XRh0UNijqO4zjnL74FneM4zpDgFbrjOM6QMLAKfaF9SoeZquUUJE1LekLSP7P/qwet63IhqSFph6Q/ZOfrsv1p92T71S7tLhbnKZJWSdos6R+Sdkt6/6jahaQ7svfj75J+LWliVO2iLgOp0GvuUzrMnF5OYT1wPfC1LP93AU9mc/ifzM5HhdtJv0I+zX3A/dk+tYdJ960dBR4E/mhmVwHvIS2TkbMLSZcC3wA2mNnVpCu9bmJ07aIWg2qhn9mn1MzawOl9SkeCPssp3AI8kok9AnxiIAouM5LWAh8DHs7OBdxEuj8tjEhZSLoQ+CDpNGHMrG1mRxhRuyCdhXdBtmnOJHCAEbSLxTCoCr1sn9KRXB8mW07hGmAbcLGZHciCXgUuHpRey8wDwLfJtwy+CDhiZqcXQB0V+1gHvAb8InM/PSxpihG0CzN7BfgB8DJpRX4U2M5o2kVtfFB0gGTLKTwKfDNcGwcgm8c/9HNKJW0EDprZ9kHrch7QBK4FfmJm1wAnidwrI2QXq0l7JuuAtwNTwM0DVepNwKAq9Dr7lA41Fcsp/FfSJVn4JcDBQem3jNwAfFzSv0ldbzeR+pFXZV1tGB372A/sN7Nt2flm0gp+FO3iw8C/zOw1M5sHHiO1lVG0i9oMqkKvs0/p0NJnOYVwb9Zbgd8vt27LjZl9x8zWmtnlpHbwlJl9Hnia9AtkGJ2yeBXYJ+nK7NKHgF2MoF2QulqulzSZvS+ny2Lk7GIxDOxL0WxVxgfI9ym9dyCKDABJHwCeAV4g9xvfTepH/x3psgp7gc+Y2aGBKDkAsmWY7zSzjZLeRdpinwZ2AF8ws/L984YISe8lHRxuAS8BXyJteI2cXUj6HvBZ0llhO4CvkPrMR84u6uKf/juO4wwJPijqOI4zJHiF7jiOMyR4he44jjMkeIXuOI4zJHiF7jiOMyR4he44jjMkeIXuOI4zJPwfT4J20q3LlaEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([300, 32, 100])\n",
      "Predicted label:  tensor(6)\n",
      "Correct label:  tensor(7)\n",
      "torch.Size([32, 3, 100, 100])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([300, 32, 100])\n",
      "Predicted label:  tensor(6)\n",
      "Correct label:  tensor(5)\n",
      "torch.Size([32, 3, 100, 100])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([300, 32, 100])\n",
      "Predicted label:  tensor(0)\n",
      "Correct label:  tensor(6)\n",
      "torch.Size([32, 3, 100, 100])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([300, 32, 100])\n",
      "Predicted label:  tensor(0)\n",
      "Correct label:  tensor(5)\n",
      "torch.Size([32, 3, 100, 100])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([300, 32, 100])\n",
      "Predicted label:  tensor(7)\n",
      "Correct label:  tensor(0)\n",
      "torch.Size([32, 3, 100, 100])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([300, 32, 100])\n",
      "Predicted label:  tensor(6)\n",
      "Correct label:  tensor(5)\n",
      "torch.Size([14, 3, 100, 100])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[-1, 32, 100]' is invalid for input of size 420000",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-130-4571af32a861>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_np\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[-1, 32, 100]' is invalid for input of size 420000"
     ]
    }
   ],
   "source": [
    "for i, (images, labels) in enumerate(test_loader):\n",
    "    print(images.shape)\n",
    "    x_np = images.numpy()\n",
    "    plt.imshow(x_np[0][:,:,1])\n",
    "    plt.show()\n",
    "    images = images.reshape(-1, sequence_length, input_size).to(device)\n",
    "    outputs = model(images.float())\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    print(images.shape)\n",
    "    print('Predicted label: ', predicted[i])\n",
    "    print('Correct label: ', labels[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7. Save the obtained results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.no_grad():\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "#     for images, labels in test_loader:\n",
    "#         images = images.reshape(-1, 300, input_size).to(device)\n",
    "#         labels = labels.to(device)\n",
    "#         outputs = model(images)\n",
    "#         _, predicted = torch.max(outputs.data, 1)\n",
    "#         total += labels.size(0)\n",
    "#         correct += (predicted == labels).sum().item()\n",
    "#     print('Test Accuracy of the model on the 10000 test images: {} %'.format(round((100 * correct / total), 2))) \n",
    "#     print('Precision: {} %'.format(round(torch.tensor(precision_score(labels,predicted, average='weighted')).item()*100, 2)))\n",
    "#     print('Recall: {} %'.format(round(torch.tensor(recall_score(labels,predicted, average='weighted')).item()*100, 2)))\n",
    "    \n",
    "\n",
    " \n",
    "\n",
    "# # Save the model checkpoint\n",
    "# torch.save(model.state_dict(), 'model.ckpt')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Test the model\n",
    "# with torch.no_grad():\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "#     for images, labels in test_loader:\n",
    "#         images = images.reshape(-1, sequence_length, input_size).to(device)\n",
    "#         labels = labels.to(device)\n",
    "#         outputs = model(images)\n",
    "#         _, predicted = torch.max(outputs.data, 1)\n",
    "#         total += labels.size(0)\n",
    "#         correct += (predicted == labels).sum().item()\n",
    "\n",
    "#     print('Test Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total)) \n",
    "\n",
    "# # Save the model checkpoint\n",
    "# torch.save(model.state_dict(), 'model.ckpt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
